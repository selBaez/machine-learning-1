\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage[compact,explicit]{titlesec}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\loglikelihood}{loglikelihood}
\newcommand{\bishop}[1]{Bishop #1}
\newcommand{\newword}[1]{{\bf #1}}
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Model}{\mathcal{M}}
\newcommand{\DataTrain}{\mathcal{D}_{{\text train}}}
\newcommand{\DataTest}{\mathcal{D}_{{\text test}}}
\newcommand{\N}{N}
\newcommand{\Ntrain}{\N_{train}}
\newcommand{\Ntest}{\N_{test}}
\newcommand{\DataSize}{\N}
\newcommand{\DataIndex}{n}
\newcommand{\cind}{t}
\newcommand{\pind}{\star}

\newcommand{\eye}{{\bf I}}

\newcommand{\Dim}{D}
\newcommand{\DimIndex}{d}

\newcommand{\DimOut}{K}
\newcommand{\DimOutIndex}{k}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\ED}{\mathbb{E}_{\Data}}
\DeclareMathOperator*{\V}{\mathbb{V}}
\DeclareMathOperator*{\LL}{L}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\trace{tr}
\DeclareMathOperator\median{median}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\expectation}{\E}
\newcommand{\expectationdata}{\ED}
\newcommand{\variance}{\V}
\newcommand{\loss}{\LL}
\newcommand{\ascalar}{a}
\newcommand{\xscalar}{x}
\newcommand{\xvec}{{\bf \xscalar}}
\newcommand{\xvectest}{\xvec_{\star}}
\newcommand{\Xmat}{{\bf \MakeUppercase\xscalar}}
\newcommand{\tscalar}{t}
\newcommand{\ttest}{\tscalar_{\star}}
\newcommand{\tvec}{{\bf \tscalar}}
\newcommand{\Tmat}{{\bf \MakeUppercase\tscalar}}
\newcommand{\yscalar}{y}
\newcommand{\yvec}{{\bf \yscalar}}
\newcommand{\Ymat}{{\bf \MakeUppercase\yscalar}}
\newcommand{\wscalar}{w}
\newcommand{\wvec}{{\bf \wscalar}}
\newcommand{\wvecML}{\wvec_{\text{MLE}}}
\newcommand{\wvecMAP}{\wvec_{\text{MAP}}}
\newcommand{\wvecs}{\wvec^{(s)}}
\newcommand{\Wmat}{{\bf \MakeUppercase\wscalar}}
\newcommand{\wbias}{\wscalar_0}
\newcommand{\xn}{\xscalar_{\DataIndex}}
\newcommand{\xvecn}{\xvec_{\DataIndex}}
\newcommand{\tvecn}{\tvec_{\DataIndex}}
\newcommand{\tn}{\tscalar_{\DataIndex}}
\newcommand{\ti}{\tscalar_{i}}
\newcommand{\yvecn}{\yvec_{\DataIndex}}
\newcommand{\yn}{\yscalar_{\DataIndex}}
\newcommand{\yfunc}{\yscalar}
\newcommand{\yfunctest}{\yfunc_{\star}}

\newcommand{\zerovec}{ {\bf 0}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\muvecN}{\muvec_{\N}}
\newcommand{\munotvec}{\boldsymbol{\mu}_0}
\newcommand{\mnot}{{\bf m_0}}
\newcommand{\mN}{{\bf m}_{\N}}
\newcommand{\mNplus}{{\bf m}_{\N+1}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\thetav}{\thetavec}
\newcommand{\phivec}{\boldsymbol{\phi}}
\newcommand{\phivectest}{\phivec_{\star}}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\phivv}{\phivec}
\newcommand{\phivecn}{\phivec_{\DataIndex}}
\newcommand{\phiveci}{\phivec_{i}}
\newcommand{\Sigmamat}{\boldsymbol{\Sigma}}
\newcommand{\SigmamatN}{\Sigmamat_N}
\newcommand{\Sigmamatnot}{\Sigmamat_0}
\newcommand{\SigmamatInv}{\Sigmamat^{-1}}
\newcommand{\Smat}{{\bf S}}
\newcommand{\SmatN}{\Smat_{\N}}
\newcommand{\Smatnot}{\Smat_0}
\newcommand{\SmatInv}{\Smat^{-1}}
\newcommand{\SmatNplus}{\Smat_{\N+1}}
\newcommand{\SmatNplusinv}{\Smat_{\N+1}^{-1}}

\newcommand{\betatest}{\beta_{\star}}
\newcommand{\Amat}{{\bf A}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}{{\bf E}}
\newcommand{\Fmat}{{\bf F}}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}{{\bf J}}
\newcommand{\Kmat}{{\bf K}}
\newcommand{\Lmat}{{\bf L}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}{{\bf N}}
\newcommand{\Omat}{{\bf O}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}{{\bf Q}}
\newcommand{\Rmat}{{\bf R}}
%\newcommand{\Smat}{{\bf S}}
%\newcommand{\Tmat}{{\bf T}}
\newcommand{\Umat}{{\bf U}}
\newcommand{\Vmat}{{\bf V}}
%\newcommand{\Wmat}{{\bf W}}
%\newcommand{\Xmat}{{\bf X}}
%\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}



\newcommand{\rvec}{{\bf r}}
\newcommand{\dvec}{{\bf d}}
\newcommand{\lvec}{{\bf l}}
\newcommand{\mvec}{{\bf m}}
\newcommand{\uvec}{{\bf u}}
\newcommand{\vvec}{{\bf v}}

\newcommand{\xvecmean}{\bar{\xvec}}
\newcommand{\xvecnest}{\tilde{\xvec}_n}
\newcommand{\xvecestn}{\xvecnest}
\newcommand{\class}{\mathcal{C}}
\newcommand{\sigmoid}{\sigma}

\newcommand{\ans}[1]{ 
	\begin{center}
		% \fbox{
		\begin{minipage}{.8\textwidth}
			{\sf#1}
		\end{minipage}
		% }
	\end{center} 
}

\title{Machine Learning 1 - Notes}
\author{Selene Baez Santamaria}

\begin{document}
	\maketitle
	\titleformat{\section}[runin]{\large\bfseries}{}{0pt}{\thesection\quad\underline{#1}\vspace{0.1in}\newline}
	\titleformat{\subsection}[runin]{\normalsize\bfseries}{}{0pt}{ \thesubsection\quad #1 \newline}
	
\vspace{0.25in}
%\section{THINGS WE NEED TO KNOW}
\centerline{\textbf{THINGS WE NEED TO KNOW}}
	\begin{enumerate}
		\item Probability:
			\begin{itemize}
				\item Variable representation
				\item Conditional probability (Bayes Theorem)
				\item Likelihood
				\item Prior
				\item Normalization constant
				\item Marginalization
			\end{itemize}
				\item Conditional models
				\begin{itemize}
					\item Objective function
					\item Maximum log-likelihood (MLE)
					\item Maximum posterior (MAP)
					\item Training ans test errors as a function of $M$
					\item Overfitting
					\item Underfitting				
				\end{itemize}
		\item Classification
			\begin{itemize}
				\item Optimal Bayesian decision rule
				\item Loss function
			\end{itemize}
		\item Regularization (Theory)
			\begin{itemize}
				\item Bias
				\item Variance
				\item Approximation of bias and variance
			\end{itemize}
		\item Conditional classification
			\begin{itemize}
				\item General expression for $p(\class_2 | x)$
				\item Conditions for classifying $x$ as $\class_1$ (inequality)
				\item Decision boundary
			\end{itemize}
		\item Regression
			\begin{itemize}
				\item Linear vs logistic functions
				\item Stochastic gradient descent algorithm
			\end{itemize}
		\item Bayesian linear regression
			\begin{itemize}
				\item Posterior distributions
				\item Likelihood 
				\item Posterior predictive distribution
			\end{itemize}
		\item General
			\begin{itemize}
				\item Gaussian distribution and its parameters
				\item Conversion from matrix to scalars
			\end{itemize}
	\end{enumerate}

\newpage

\section{Intro}
	
\subsection{Notation}
	\begin{enumerate}
		\item $N$: number of data samples
		\item $D$: dimensionality of input data
		\item $K$: number of output predictions (labels or tags)
		\item $x_n$: row vector for $n^{th}$ sample
	\end{enumerate}
	
\subsection{Models, Algorithms, etc}
	A model:
	\begin{itemize}
		\item Belongs to a \texttt{model class}, with corresponding \texttt{complexity}
		\item Depends on \texttt{parameters} $\theta$
		\item is optimized by minimizing/maximizing an \texttt{objective function}
		
	\end{itemize} 


	
\end{document}