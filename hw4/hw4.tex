\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[compact,explicit]{titlesec}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{subfigure}
\newcommand\ie{\textit{i.e.\ }}
\newcommand\eg{\textit{e.g.\ }}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\loglikelihood}{loglikelihood}
\newcommand{\bishop}[1]{Bishop #1}
\newcommand{\newword}[1]{{\bf #1}}
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Model}{\mathcal{M}}
\newcommand{\DataTrain}{\mathcal{D}_{{\text train}}}
\newcommand{\DataTest}{\mathcal{D}_{{\text test}}}
\newcommand{\N}{N}
\newcommand{\Ntrain}{\N_{train}}
\newcommand{\Ntest}{\N_{test}}
\newcommand{\DataSize}{\N}
\newcommand{\DataIndex}{n}
\newcommand{\cind}{t}
\newcommand{\pind}{\star}

\newcommand{\eye}{{\bf I}}

\newcommand{\Dim}{D}
\newcommand{\DimIndex}{d}

\newcommand{\DimOut}{K}
\newcommand{\DimOutIndex}{k}



\newcommand{\expectation}{\E}
\newcommand{\expectationdata}{\ED}
\newcommand{\variance}{\V}
\newcommand{\loss}{\LL}
\newcommand{\ascalar}{a}
\newcommand{\xscalar}{x}
\newcommand{\xvec}{{\bf \xscalar}}
\newcommand{\xvectest}{\xvec_{\star}}
\newcommand{\Xmat}{{\bf \MakeUppercase\xscalar}}
\newcommand{\tscalar}{t}
\newcommand{\ttest}{\tscalar_{\star}}
\newcommand{\tvec}{{\bf \tscalar}}
\newcommand{\Tmat}{{\bf \MakeUppercase\tscalar}}
\newcommand{\yscalar}{y}
\newcommand{\yvec}{{\bf \yscalar}}
\newcommand{\Ymat}{{\bf \MakeUppercase\yscalar}}
\newcommand{\wscalar}{w}
\newcommand{\wvec}{{\bf \wscalar}}
\newcommand{\wvecML}{\wvec_{\text{MLE}}}
\newcommand{\wvecMAP}{\wvec_{\text{MAP}}}
\newcommand{\wvecs}{\wvec^{(s)}}
\newcommand{\Wmat}{{\bf \MakeUppercase\wscalar}}
\newcommand{\wbias}{\wscalar_0}
\newcommand{\xn}{\xscalar_{\DataIndex}}
\newcommand{\xvecn}{\xvec_{\DataIndex}}
\newcommand{\tvecn}{\tvec_{\DataIndex}}
\newcommand{\tn}{\tscalar_{\DataIndex}}
\newcommand{\ti}{\tscalar_{i}}
\newcommand{\yvecn}{\yvec_{\DataIndex}}
\newcommand{\yn}{\yscalar_{\DataIndex}}
\newcommand{\yfunc}{\yscalar}
\newcommand{\yfunctest}{\yfunc_{\star}}

\newcommand{\zerovec}{ {\bf 0}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\muvecN}{\muvec_{\N}}
\newcommand{\munotvec}{\boldsymbol{\mu}_0}
\newcommand{\mnot}{{\bf m_0}}
\newcommand{\mN}{{\bf m}_{\N}}
\newcommand{\mNplus}{{\bf m}_{\N+1}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\thetav}{\thetavec}
\newcommand{\phivec}{\boldsymbol{\phi}}
\newcommand{\phivectest}{\phivec_{\star}}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\phivv}{\phivec}
\newcommand{\phivecn}{\phivec_{\DataIndex}}
\newcommand{\phiveci}{\phivec_{i}}
\newcommand{\Sigmamat}{\boldsymbol{\Sigma}}
\newcommand{\SigmamatN}{\Sigmamat_N}
\newcommand{\Sigmamatnot}{\Sigmamat_0}
\newcommand{\SigmamatInv}{\Sigmamat^{-1}}
\newcommand{\Smat}{{\bf S}}
\newcommand{\SmatN}{\Smat_{\N}}
\newcommand{\Smatnot}{\Smat_0}
\newcommand{\SmatInv}{\Smat^{-1}}
\newcommand{\SmatNplus}{\Smat_{\N+1}}
\newcommand{\SmatNplusinv}{\Smat_{\N+1}^{-1}}

\newcommand{\betatest}{\beta_{\star}}
\newcommand{\Amat}{{\bf A}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}{{\bf E}}
\newcommand{\Fmat}{{\bf F}}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}{{\bf J}}
\newcommand{\Kmat}{{\bf K}}
\newcommand{\Lmat}{{\bf L}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}{{\bf N}}
\newcommand{\Omat}{{\bf O}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}{{\bf Q}}
\newcommand{\Rmat}{{\bf R}}
%\newcommand{\Smat}{{\bf S}}
%\newcommand{\Tmat}{{\bf T}}
\newcommand{\Umat}{{\bf U}}
\newcommand{\Vmat}{{\bf V}}
%\newcommand{\Wmat}{{\bf W}}
%\newcommand{\Xmat}{{\bf X}}
%\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}

\newcommand{\rvec}{{\bf r}}
\newcommand{\dvec}{{\bf d}}
\newcommand{\lvec}{{\bf l}}
\newcommand{\mvec}{{\bf m}}
\newcommand{\uvec}{{\bf u}}
\newcommand{\vvec}{{\bf v}}

\newcommand{\xvecmean}{\bar{\xvec}}
\newcommand{\xvecnest}{\tilde{\xvec}_n}
\newcommand{\xvecestn}{\xvecnest}
\newcommand{\class}{\mathcal{C}}
\newcommand{\sigmoid}{\sigma}

\newcommand{\avec}{\mathbf{a}}
\newcommand{\xivec}{\boldsymbol{\xi}}

\title{Machine Learning 1 - Homework 4}
\author{Selene Baez Santamaria}

%\author{Monday, October 03, 2016 \\ Deadline: Wednesday, October 12, 2016, 23:59}
\date{}
\begin{document}
\maketitle
\titleformat{\section}[runin]{\large\bfseries}{}{0pt}{\thesection\quad\underline{#1}\vspace{0.1in}\newline}
\titleformat{\subsection}[runin]{\normalsize\bfseries}{}{0pt}{#1 \thesubsection\newline}


\section{Lagrange Multipliers}

In this exercise, we will do optimization problems using Lagrange Multipliers. Suppose we would like to maximize the function
\begin{align}
  f(\mathbf{x}) = 1 - x_1^2 - 2x_2^2
\end{align}


{\bf Answer the following questions:}

\begin{enumerate}
\item Find the maximum of $1 - x_1^2 - 2x_2^2$, subject to the constraint that $x_1+x_2=1$. \\
	\emph{Solution:} \\
		Lagrangian:
			\begin{align*}
			  	L = 1 - x_1^2 -2x_2^2 + \lambda (x_1 + x_2 - 1)
			\end{align*}
		Partial derivatives:
			\begin{align}
				\frac{\partial L}{\partial x_1} &= -2x_1 + \lambda = 0 \\
				\frac{\partial L}{\partial x_2} &= -4x_2 + \lambda = 0 \\
				\frac{\partial L}{\partial \lambda} &= x_1 + x_2 - 1 = 0
			\end{align}
		From 2 and 3 we have: 
			\begin{align*}
				x_1 &= 2x_2
			\end{align*}
		Substituting in 4:
			\begin{align*}
				3x_2 &= 1 \\
			\end{align*}
		Therefore:
			\begin{align*}
			x_1 &= \frac{2}{3} &
			x_2 &= \frac{1}{3} &
			\lambda &=  \frac{4}{3}
			\end{align*}

\item Find the maximum of $1-x_1^2 -x_2^2$ subject to the constraint $x_1+x_2-1\geqslant 0$ \\
	\emph{Solution:} \\
		Lagrangian:
			\begin{align*}
			L = 1 - x_1^2 - x_2^2 + \lambda (x_1 + x_2 - 1)
			\end{align*}
		Partial derivatives:
			\begin{align}
			\frac{\partial L}{\partial x_1} &= -2x_1 + \lambda = 0 \\
			\frac{\partial L}{\partial x_2} &= -2x_2 + \lambda = 0 \\
			\frac{\partial L}{\partial \lambda} &= x_1 + x_2 - 1 \geqslant 0
			\end{align}
		Additional constraints:
			\begin{align}
				\lambda &\geqslant 0 \\
				\lambda (x_1 + x_2 - 1) &= 0
			\end{align}
		From 5 and 6 we have: 
			\begin{align*}
			x_1 &= x_2 \\
			x_1 &= \frac{\lambda}{2}
			\end{align*}
		Substituting in 9:
			\begin{align*}
			2x_2 &= 1
			\end{align*}
		Therefore:
			\begin{align*}
			x_1 &= \frac{1}{2} &
			x_2 &= \frac{1}{2} &
			\lambda &=  1
			\end{align*}
		Checking the constraints we note that 7 and 8 hold, hence the solution satisfies all the conditions.

\item Find the maximum of $1-x_1^2 -x_2^2$ subject to the constraint $-x_1-x_2+1\geqslant 0$ \\
	\emph{Solution:} \\
		Lagrangian:
			\begin{align*}
			L = 1 - x_1^2 - x_2^2 + \lambda (-x_1 - x_2 + 1)
			\end{align*}
		Partial derivatives:
			\begin{align}
			\frac{\partial L}{\partial x_1} &= -2x_1 - \lambda = 0 \\
			\frac{\partial L}{\partial x_2} &= -2x_2 - \lambda = 0 \\
			\frac{\partial L}{\partial \lambda} &= -x_1 - x_2 + 1 \geqslant 0
			\end{align}
		Additional constraints:
			\begin{align}
			\lambda &\geqslant 0 \\
			\lambda (-x_1 - x_2 + 1) &= 0
			\end{align}
		From 10 and 11 we have: 
			\begin{align*}
			x_1 &= x_2 \\
			x_1 &= - \frac{\lambda}{2}
			\end{align*}
		Substituting in 12:
			\begin{align*}
			2x_2 &= 1
			\end{align*}
		Therefore:
			\begin{align*}
			x_1 &= \frac{1}{2} &
			x_2 &= \frac{1}{2} &
			\lambda &=  -1
			\end{align*}
		Checking the constraints we note that 13 does not hold, hence we need to revisit the solution for 12. Choosing the first term this time: 
			\begin{align*}
			\lambda &= 0
			\end{align*}
		Therefore:
			\begin{align*}
			x_1 &= 0 &
			x_2 &= 0 &
			\lambda &=  0
			\end{align*}
		One more check at the constraints shows that 12 and 13 hold, hence the solution satisfies all the conditions.


\item Find the maximum of $x_1 + 2x_2 - 2x_3$, subject to the constraint  that $x_1^2+x_2^2+x_3^2 = 1$. \\
	\emph{Solution:} \\
		Lagrangian:
			\begin{align*}
			L = x_1 + 2x_2 - 2x_3 + \lambda (x_1^2 + x_2^2 + x_3^2 - 1)
			\end{align*}
		Partial derivatives:
			\begin{align}
			\frac{\partial L}{\partial x_1} &= 1 + 2\lambda x_1 = 0 \\
			\frac{\partial L}{\partial x_2} &= 2 + 2\lambda x_2 = 0 \\
			\frac{\partial L}{\partial x_3} &= -2 + 2\lambda x_3 = 0 \\
			\frac{\partial L}{\partial \lambda} &= x_1^2+x_2^2+x_3^2 - 1 = 0
			\end{align}
		From 15, 16 and 17 we have: 
			\begin{align*}
			\lambda = - \frac{1}{2x_1} &= - \frac{1}{x_2} = \frac{1}{x_3} \\
			&\therefore \\
			x_2 = 2x_1 &, 
			x_3 = -2x_1
			\end{align*}
		Substituting in 18:
			\begin{align*}
			x_1^2+ (2x_1)^2+ (-2x_1)^2 - 1 &= 0 \\
			x_1^2 = \frac{1}{9}
			\end{align*}
		Therefore:
			\begin{align*}
			x_1 &= \frac{1}{3} &
			x_2 &= \frac{2}{3} &
			x_3 &= -\frac{2}{3} &
			\lambda &=  - \frac{3}{2}
			\end{align*}
		Or
			\begin{align*}
			x_1 &= -\frac{1}{3} &
			x_2 &= -\frac{2}{3} &
			x_3 &= \frac{2}{3} &
			\lambda &=  \frac{3}{2}
			\end{align*}
		In order to select the appropriate set of values, we substitute in the original function and choose the maximum:
			\begin{align}
			x_1 + 2x_2 - 2x_3 &= \frac{1}{3} + 2(\frac{2}{3}) - 2(-\frac{2}{3}) = \frac{9}{3} = 3 \\
			x_1 + 2x_2 - 2x_3 &= -\frac{1}{3} + 2(-\frac{2}{3}) - 2(\frac{2}{3}) = -\frac{9}{3} = -3
			\end{align}
		Since 19 has the maximum, we select the first set of values.

\item A company manufactures a chemical product out of two ingredients, known as ingredient X and ingredient Y. The number of doses produced, $D$, is given by $6x^{2/3}y^{1/2}$, where $x$ and $y$ are the number of grams of ingredients X and Y respectively. Suppose ingredient X consts {4} euro per gram, and ingredient Y costs 3 euro per gram. Find out the maximum number of doses that can be made if no more than 7000 euro can be spent on the ingredients.\\
	\emph{Solution:} \\
		Constraint:
			\begin{align*}
				4x + 3y \leqslant 7000
			\end{align*}
		Lagrangian:
			\begin{align*}
			L = 6x^{2/3}y^{1/2} + \lambda (7000 -4x -3y)
			\end{align*}
		Partial derivatives:
			\begin{align}
			\frac{\partial L}{\partial x} &= 4x^{-1/3}y^{1/2} - 4\lambda = 0 \\
			\frac{\partial L}{\partial y} &= 3x^{2/3}y^{-1/2} - 3\lambda = 0 \\
			\frac{\partial L}{\partial \lambda} &= 7000 -4x -3y \geqslant 0
			\end{align}
		Additional constraints:
			\begin{align}
			\lambda &\geqslant 0
			\end{align}
		From 21 and 22 we have: 
			\begin{align*}
			\lambda = x^{-1/3}y^{1/2} &= x^{2/3}y^{-1/2} \\
			&\therefore \\
			x &= y
			\end{align*}
		Substituting in 23:
			\begin{align*}
			7000 &= 7x \\
			x &= 1000
			\end{align*}
		Therefore:
			\begin{align*}
			x &= 1000 &
			y &= 1000 &
			\lambda &=  1000^{1/6}
			\end{align*}
		Checking the constraints we note that 24 holds, hence the solution satisfies all the conditions.
\end{enumerate}

\setcounter{equation}{0}
\section{Kernel Outlier Detection}
Our task is to derive an algorithm that will detect the outliers (in this example there are $2$ of them). To that end, we draw a circle rooted at location $\boldsymbol{a}$ and with radius $R$. All data-cases that fall outside the circle are detected as outliers.

We will now write down the primal program that will find such a circle:
\begin{align*}
&\min_{\boldsymbol{a},R,\boldsymbol{\xi}} R^2+C \sum_{i=1}^N \xi_i\\
s.t.~\forall i : &\| \boldsymbol{x_i} - \boldsymbol{a} \|^2 \le R^2 + \xi_i,~\xi_i \ge 0
\end{align*}

In words: we want to minimize the radius of the circle subject to the constraint that most data-cases should lay inside it. Outliers are allowed to stay outside but they pay a price proportional their distance from the circle boundary and $C$.

{\bf Answer the following questions:}

\begin{enumerate}
\item Introduce Lagrange multipliers for the constraints and write down the primal Lagrangian. Use the following notation: $\{\alpha_i\}$ are the Lagrange multipliers for the first constraint and $\{\mu_i\}$ for the second constraint.\\
	\emph{Solution:} \\
		Given that $\alpha_i$ and $\mu_i$ are Lagrange multipliers:
			\begin{align*}
			L &= R^2 + C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 - R^2 - \xi_i \rp - \sum_i^N \mu_i \xi_i
			\end{align*}

\item Write down all KKT conditions. (Hint: take the derivative w.r.t. $R^2$ instead of $R$). \\
	\emph{Solution:} \\
		Partial derivatives:
			\begin{align}
				\frac{\partial L}{\partial R^2} &= 1 - \sum_i^N \alpha_i = 0 \\
				\frac{\partial L}{\partial \xi_i} &= C - \alpha_i - \mu_i = 0 \\
				\frac{\partial L}{\partial \avec} &= -2 \sum_i^N \alpha_i ( \xvec_i - \avec)  = 0
			\end{align}
		Additional constraints:
			\begin{align}
				||\xvec_i - \avec||^2 - R^2 - \xi_i &\leq 0 \\
				\xi_i &\geq 0 \\
				\alpha_i &\geq 0 \\
				\mu_i &\geq 0 \\
				\mu_i \cdot \xi_i &= 0 \\
				\alpha_i \cdot \lp ||\xvec_i - \avec||^2 - R^2 - \xi_i \rp &= 0
			\end{align}


\item Identify the complementary slackness conditions. Use these conditions to derive what data-cases will have $\alpha_i>0$ (support vectors) and which ones will have $\mu_i>0$. \\
	\emph{Solution:} \\
		7 and 9 are the complementary slackness conditions. 
		Considering $\alpha_i$
			\begin{itemize}
				\item Data cases inside the circle: \\
					$||\xvec_i - \avec||^2 - R^2 - \xi_i < 0$ \\
					$\therefore \alpha_i = 0$
				\item Data cases on or outside the circle: \\
					$||\xvec_i - \avec||^2 - R^2 - \xi_i = 0$ \\
					$\therefore \alpha_i \geqslant 0$
			\end{itemize}

		Considering for $\mu_i$
			\begin{itemize}
				\item Data cases on or inside the circle: \\
				$\xi_i = 0 \therefore \mu_i \geqslant 0$
				\item Data cases outside the circle: \\
				$\xi_i > 0 \therefore \mu_i = 0$
			\end{itemize}

\item Derive the dual Lagrangian and specify the dual optimization problem. Kernelize the problem, i.e. write the dual program only in terms of kernel entries and Lagrange multipliers. \\
	\emph{Solution:} \\
		From 1, 2, and 3, we have:
			\begin{align}
			\sum_i^N \alpha_i = 1 \\
			\mu_i = C - \alpha_i \\
			\avec = \sum_i^N \alpha_i \xvec_i 
			\end{align}
		Hereby we rewrite the Lagrangian:
			\begin{align*}
			\text{Original Lagranian:} \\
			L &= R^2 + C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 - R^2 - \xi_i \rp - \sum_i^N \mu_i \xi_i \\
			\text{Expand:} \\
			L &= R^2 + C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 \rp - \sum_i^N \alpha_i R^2 - \sum_i^N \alpha_i \xi_i - \sum_i^N \mu_i \xi_i \\
			\text{Using 10:} \\
			L &= R^2 + C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 \rp - R^2 - \sum_i^N \alpha_i \xi_i - \sum_i^N \mu_i \xi_i \\
			\text{Cancel } R^2 \\
			L &= C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 \rp - \sum_i^N \alpha_i \xi_i - \sum_i^N \mu_i \xi_i \\
			\text{Using 11:} \\
			L &= C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 \rp - \sum_i^N \alpha_i \xi_i - \sum_i^N (C - \alpha_i) \xi_i \\
			\text{Expand:} \\
			L &= C \sum_i^N \xi_i + \sum_i^N \alpha_i \lp ||\xvec_i - \avec||^2 \rp - \sum_i^N \alpha_i \xi_i - \sum_i^N C \xi_i + \sum_i^N \alpha_i \xi_i \\
			\text{Cancel terms:} \\
			L &= \sum_i^N \alpha_i ||\xvec_i - \avec||^2 \\
			\end{align*}
			
			\begin{align*}
			L &= \sum_i^N \alpha_i \lp \xvec_i^T\xvec + \avec^T\avec - 2\xvec_i^T \avec \rp \\
			L &= \sum_i^N \alpha_i \xvec_i^T\xvec_i + 
			\sum_i^N \alpha_i  \avec^T\avec -2 \sum_i^N \alpha_i \xvec_i^T \avec \\
			L &= \sum_i^N \alpha_i \xvec_i^T\xvec_i + \avec^T\avec -2 \avec^T \avec \\
			L &= \sum_i^N \alpha_i \xvec_i^T\xvec_i - \avec^T\avec \\
			L &= \sum_i^N \alpha_i K(\xvec_i, \xvec_i) -  \sum_{i,j}^N \alpha_i \alpha_j K(\xvec_i, \xvec_j)  \\
			\end{align*}
		Hence the dual program becomes:
			\begin{align*}
			&\max_{\alpha} \sum_i^N \alpha_i K(\xvec_i, \xvec_i) -  \sum_{i,j}^N \alpha_i \alpha_j K(\xvec_i, \xvec_j) \\
			& s.t.  0 \leq a_i \leq C, \forall i
			\end{align*}
	
\item The dual program will return optimal values for $\{\alpha_i\}$. In terms of these, compute the optimal values for the other dual variables $\{\mu_i\}$.

Then, solve the primal variables $\{\boldsymbol{a},R,\boldsymbol{\xi} \}$ (in that order) in terms of the dual variables $\{\mu_i,\alpha_i\}$. Note that you do not need to know the dual optimization program to solve this question. You only need the KKT conditions.  \\
	\emph{Solution:} \\
		\begin{itemize}
			\item Optimal values of $\alpha_i$ \\
				$\alpha_i^\star$ Given from dual program
			
			\item Optimal values of $\mu_i$ \\
				From 11: $\mu_i^\star = C - \alpha_i^\star $
			
			\item Optimal values of $\avec$ \\
				From 3: 
				\begin{align*}
				& - \sum_i^N \alpha_i^\star ( \xvec_i - \avec ) = 0 \\
				& - \sum_i^N \alpha_i^\star \xvec_i + \sum_i^N \alpha_i^\star \avec  = 0 \\
				&\avec = \frac{\sum_i^N \alpha_i^\star \xvec_i}{\sum_i^N \alpha_i^\star} \\
				&\avec = \sum_i^N \alpha_i^\star \xvec_i
				\end{align*}
			
			\item Optimal values of $R$ \\
				The optimal value for $R$ is determined by $\xvec_i$.
				
				\begin{equation*}
				R = \begin{cases}
				||\xvec_i - \avec||^2 & \quad \text{if } \xvec_i \text{ is on the circle}\\
				\frac{1}{N_{ball}}\sum_i^N ||\xvec_i - \avec^\star||^2  & \quad \text{if } \xvec_i \text{ otherwise}\\
				\end{cases}
				\end{equation*}
			
			\item Optimal values of $\boldsymbol{\xi}$ \\
				Similarly, the optimal value for $\xi_i$ is determined by $\xvec_i$.
				
				\begin{equation*}
				\xi_i = \begin{cases}
				0      & \quad \text{if } \xvec_i \text{ is on or inside the circle}\\
				||\xvec_i - \avec^\star||^2 - R^2  & \quad \text{if } \xvec_i \text{ is outside the circle}\\
				\end{cases}
				\end{equation*}
		\end{itemize}


\item Assume we have solved the dual program. We now want to apply it to new test cases. Describe a test in the dual space (i.e. in terms if kernels and Lagrange multipliers) that could serve to detect outliers. (Students who got stuck along the way may describe the test in primal space). \\
\emph{Solution:} \\
\begin{align*}
&\||\xvec^* - \avec||^2 > R^2 \\
\text{Expand:} \\
&\||\xvec^* - \sum_i^N \alpha_i \xvec_i||^2 > R^2 \\
\text{Norm identity:} \\
&(\xvec^* - \sum_i^N \alpha_i \xvec_i)^T(\xvec^* - \sum_i^N \alpha_i \xvec_i) > R^2 \\
&{\xvec^*}^T\xvec^* + \sum_{i,j}^N \alpha_i \alpha_j \xvec_i^T \xvec_j - 2 {\xvec^*}^T \sum_i^N \alpha_i \xvec_i > R^2 \\
&\text{If } K(\xvec^*, \xvec^*) + \sum_{i,j}^N \alpha_i \alpha_j K(\xvec_i, \xvec_j) - 2 \sum_i^N \alpha_i K(\xvec^*, \xvec_i) > R^2 \\
&\text{  then the point is an outlier} \\
\end{align*}

\item What kind of solution do you expect if we use $C=0$. And what solution if we use $C=\infty$? \\
\emph{Solution:} \\
If $C \to 0$ the cost of a variable being outside of the circle is nothing. Therefore the values of $\xi_i$ can become as large as necessary without any penalty and the minimization plainly minimizes $R^2$. Thus, $R$ will approach zero if $C \to 0$.

If $C\to\infty$ the cost of a variable being outside the circle is infinitely large. To minimize the function, $R$ is made large enough to accommodate every data point. In other words, every data point lies in the circle. Because the cost of a data point being outside the bounds is so large, R is made large enough so that it encircles all data points.

\item Describe geometrically what kind of solutions we may expect if we use a RBF kernel (Gaussian) with very small bandwidth (sigma = small), i.e.~describe how these solutions can be different geometrically (in x-space) from the case with a linear kernel. \\
\emph{Solution:} \\
The output of an RBF kernel is high if two points are alike and low if two points are apart. An RBF kernel is therefore able to make clusters of data, not limited to a single circle. It actually models data points on how close they are. 


\item Now assume that you are given labels (e.g. y=1 for outlier and y=-1 for ``inlier'').  Change the primal problem to include these labels and turn it into a classification problem similar to the SVM. (You do not have to derive the dual program). \\
\emph{Solution:} \\
Restating the original optimization problem.

\begin{align*}
\min_{\avec, R, \xivec} R^2 &+ C \sum_i^N \xi_i \\
s.t. \forall i : ||\xvec_i - \avec||^2 &\leq R^2 + \xi_i, \xi_i \geq 0
\end{align*}

Now it is assumed that there is some boundary at distance R that separates inliers from outliers. With $y = -1$ for inliers and $y = 1$ for outliers. The optimization problem is now defined as depicted below. Data points are assumed to be distanced $1$ away from the separation boundary. The problem is now described as a two-sided SVM.

\begin{align*}
\text{Our objective is } \min_{\avec, R, \xivec} R^2 &+ C \sum_i^N \xi_i \\
y (||\xvec_i - \avec||^2 &- R^2) > 1 - \xi_i  \\
s.t. \: \forall i : y (||\xvec_i - \avec||^2 &- R^2) - 1 + \xi_i \geq 0 \\
for \enspace \xi_i \geq 0
\end{align*}

\end{enumerate}


\end{document}