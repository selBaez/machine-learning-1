\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage[compact,explicit]{titlesec}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\loglikelihood}{loglikelihood}
\newcommand{\bishop}[1]{Bishop #1}
\newcommand{\newword}[1]{{\bf #1}}
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Model}{\mathcal{M}}
\newcommand{\DataTrain}{\mathcal{D}_{{\text train}}}
\newcommand{\DataTest}{\mathcal{D}_{{\text test}}}
\newcommand{\N}{N}
\newcommand{\Ntrain}{\N_{train}}
\newcommand{\Ntest}{\N_{test}}
\newcommand{\DataSize}{\N}
\newcommand{\DataIndex}{n}
\newcommand{\cind}{t}
\newcommand{\pind}{\star}

\newcommand{\eye}{{\bf I}}

\newcommand{\Dim}{D}
\newcommand{\DimIndex}{d}

\newcommand{\DimOut}{K}
\newcommand{\DimOutIndex}{k}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\ED}{\mathbb{E}_{\Data}}
\DeclareMathOperator*{\V}{\mathbb{V}}
\DeclareMathOperator*{\LL}{L}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\trace{tr}
\DeclareMathOperator\median{median}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\expectation}{\E}
\newcommand{\expectationdata}{\ED}
\newcommand{\variance}{\V}
\newcommand{\loss}{\LL}
\newcommand{\ascalar}{a}
\newcommand{\xscalar}{x}
\newcommand{\xvec}{{\bf \xscalar}}
\newcommand{\xvectest}{\xvec_{\star}}
\newcommand{\Xmat}{{\bf \MakeUppercase\xscalar}}
\newcommand{\tscalar}{t}
\newcommand{\ttest}{\tscalar_{\star}}
\newcommand{\tvec}{{\bf \tscalar}}
\newcommand{\Tmat}{{\bf \MakeUppercase\tscalar}}
\newcommand{\yscalar}{y}
\newcommand{\yvec}{{\bf \yscalar}}
\newcommand{\Ymat}{{\bf \MakeUppercase\yscalar}}
\newcommand{\wscalar}{w}
\newcommand{\wvec}{{\bf \wscalar}}
\newcommand{\wvecML}{\wvec_{\text{MLE}}}
\newcommand{\wvecMAP}{\wvec_{\text{MAP}}}
\newcommand{\wvecs}{\wvec^{(s)}}
\newcommand{\Wmat}{{\bf \MakeUppercase\wscalar}}
\newcommand{\wbias}{\wscalar_0}
\newcommand{\xn}{\xscalar_{\DataIndex}}
\newcommand{\xvecn}{\xvec_{\DataIndex}}
\newcommand{\tvecn}{\tvec_{\DataIndex}}
\newcommand{\tn}{\tscalar_{\DataIndex}}
\newcommand{\ti}{\tscalar_{i}}
\newcommand{\yvecn}{\yvec_{\DataIndex}}
\newcommand{\yn}{\yscalar_{\DataIndex}}
\newcommand{\yfunc}{\yscalar}
\newcommand{\yfunctest}{\yfunc_{\star}}

\newcommand{\zerovec}{ {\bf 0}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\muvecN}{\muvec_{\N}}
\newcommand{\munotvec}{\boldsymbol{\mu}_0}
\newcommand{\mnot}{{\bf m_0}}
\newcommand{\mN}{{\bf m}_{\N}}
\newcommand{\mNplus}{{\bf m}_{\N+1}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\thetav}{\thetavec}
\newcommand{\phivec}{\boldsymbol{\phi}}
\newcommand{\phivectest}{\phivec_{\star}}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\phivv}{\phivec}
\newcommand{\phivecn}{\phivec_{\DataIndex}}
\newcommand{\phiveci}{\phivec_{i}}
\newcommand{\Sigmamat}{\boldsymbol{\Sigma}}
\newcommand{\SigmamatN}{\Sigmamat_N}
\newcommand{\Sigmamatnot}{\Sigmamat_0}
\newcommand{\SigmamatInv}{\Sigmamat^{-1}}
\newcommand{\Smat}{{\bf S}}
\newcommand{\SmatN}{\Smat_{\N}}
\newcommand{\Smatnot}{\Smat_0}
\newcommand{\SmatInv}{\Smat^{-1}}
\newcommand{\SmatNplus}{\Smat_{\N+1}}
\newcommand{\SmatNplusinv}{\Smat_{\N+1}^{-1}}

\newcommand{\betatest}{\beta_{\star}}
\newcommand{\Amat}{{\bf A}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}{{\bf E}}
\newcommand{\Fmat}{{\bf F}}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}{{\bf J}}
\newcommand{\Kmat}{{\bf K}}
\newcommand{\Lmat}{{\bf L}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}{{\bf N}}
\newcommand{\Omat}{{\bf O}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}{{\bf Q}}
\newcommand{\Rmat}{{\bf R}}
%\newcommand{\Smat}{{\bf S}}
%\newcommand{\Tmat}{{\bf T}}
\newcommand{\Umat}{{\bf U}}
\newcommand{\Vmat}{{\bf V}}
%\newcommand{\Wmat}{{\bf W}}
%\newcommand{\Xmat}{{\bf X}}
%\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}



\newcommand{\rvec}{{\bf r}}
\newcommand{\dvec}{{\bf d}}
\newcommand{\lvec}{{\bf l}}
\newcommand{\mvec}{{\bf m}}
\newcommand{\uvec}{{\bf u}}
\newcommand{\vvec}{{\bf v}}

\newcommand{\xvecmean}{\bar{\xvec}}
\newcommand{\xvecnest}{\tilde{\xvec}_n}
\newcommand{\xvecestn}{\xvecnest}
\newcommand{\class}{\mathcal{C}}
\newcommand{\sigmoid}{\sigma}

\newcommand{\ans}[1]{ 
  \begin{center}
    % \fbox{
      \begin{minipage}{.8\textwidth}
        {\sf#1}
      \end{minipage}
    % }
  \end{center} 
}

\title{Machine Learning 1 - Homework 2}
\author{Selene Baez Santamaria}

%\author{Monday, September 12, 2016 \\ Deadline: Wednesday, September 21, 2016, 23:59}
\date{}
\begin{document}
\maketitle
\titleformat{\section}[runin]{\large\bfseries}{}{0pt}{\thesection\quad\underline{#1}\vspace{0.1in}\newline}
\titleformat{\subsection}[runin]{\normalsize\bfseries}{}{0pt}{#1 \thesubsection\newline}

\vspace{0.25in}
\section{MAP solution for Linear Regression}
In class we solved for the maximum likelihood estimator for linear regression with polynomial basis functions.  In this exercise you will solve for the {\em maximum a posterior} (MAP) solution: $\wvecMAP = \lp \Phimat^T \Phimat + \lambda \eye\rp^{-1} \Phimat^T \tvec$.  For this problem we assume $N$ training vectors $\{ \xvecn \}_{n=1}^N$, each of which is mapped using basis functions to $\phivecn$.  In the training set, the data come in input-output pairs, i.e. $\{\xvecn, \tn\}$.  We assume that one of the basis functions is the constant $1$, and there are $M-1$ other basis function in $\phivecn$.  We also have the following information:
\begin{itemize}
  \item The regression prediction: $\yfunc(\xvecn, \wvec) = \wvec^T\phivecn$.
  \item The likelihood function: $p( \tn | \phivecn, \wvec, \beta ) = \mathcal{N}\lp \tn | \wvec^T\phivecn, 1/\beta \rp$
  \item The prior over $\wvec$: $p(\wvec) = \mathcal{N}\lp \wvec | \zerovec, \eye/\alpha\rp$.  $\eye$ is the identity matrix, $\zerovec$ is a vector of $0$'s.
  \item The data are iid (independently and identically distributed).
\end{itemize}
Answer the following:
\begin{enumerate}
  \item Write down the likelihood $p(\Data | \thetav)$ using a) a product over $N$ and b) in vector/matrix form.    Tip: You can answer both a) and b) in one set of equations by starting with a), then simplifying to get b).  For b) make sure to define any matrices and vectors. \\
	  \emph{Solution:} \\
		  Given that the likelihood function for a single training vector is a normal distribution, we can write its product as: \\
		  
		  \begin{align*}
		  p(\Data | \thetav) &= p(\tvec | \Phimat, \wvec, \beta) \\
		  &= \prod_{n = 0}^N p(\tn | \phivecn, \wvec, \beta) \\
		  &= \prod_{n=1}^{N} \mathcal{N}(\tn| \wvec^T \phivecn, 1 / \beta) \\
		  &=  \prod_{n=1}^{N} \frac{\beta^{1/2}}{(2\pi)^{1/2}} \exp (-\frac{\beta}{2}(\tn- \wvec^T \phivecn)^2)\\
		  \end{align*}
		  
		  In order to reach a vector form, we solve the product: \\
		  
		  \begin{align*}         
		  p(\Data | \thetav) &=  \frac{\beta^{N/2}}{(2\pi)^{N/2}} \prod_{n=1}^{N} \exp (-\frac{\beta}{2}(\tn- \wvec^T \phivecn)^2)\\    
		  &=  \underbrace{\frac{\beta^{N/2}}{(2\pi)^{N/2}}}_{\text{constant term}} \exp (-\frac{\beta}{2}\sum(\tn - \wvec^T \phivecn)^2)\\ 
		  \end{align*}
		  
		  Next, Using the fact that $\sum_i x_i^2 = \mathbf{x}^T\mathbf{x}$, we transform the $\tn$ into a vector, and $\phivecn$ into a matrix, where \\
		  
		  \begin{equation*}
		  \Phimat = 
		  \begin{pmatrix}
		  - \phivec_1 - \\
		  - \phivec_2 - \\
		  \vdots  \\
		  - \phivec_N -\\
		  \end{pmatrix}
		  \end{equation*}
		  
		  Thus: \\
		  
		  \begin{align*}
		  p(\Data | \thetav) &=  \frac{\beta^{N/2}}{(2\pi)^{N/2}} \exp (-\frac{\beta}{2}(\tvec - \Phimat \wvec)^T(\tvec - \Phimat \wvec))\\
		  &=\mathcal{N}(\tvec | \Phimat \wvec, \frac{1}{\beta} \eye )
		  \end{align*}
		  
		  We can see that the product is also a Normal disribution. \\
		  
	  
  \item Write down the prior $p(\wvec)$ (by expanding the expression for multivariate Gaussian distribution).  Compute its log.\\
	  \emph{Solution:} \\
		  The prior $p(\wvec)$ is given by a normal distribution with $\mu = 0$ and $\sigma = \frac{1}{\alpha}\eye$. Expanding over the general formula of the Normal Distribution we get: \\
		  
		  \begin{align*}
		  p(\wvec) &= \mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha}\eye) \\
		  &= \frac{\alpha^{D/2}}{(2\pi)^{D/2}} \exp (- \frac{\alpha}{2} \wvec^T \wvec)
		  \end{align*}
		  
		  Where $\mathcal{D}$ is the dimensionality of \textbf{w}. \\
		  
		  Next, because $\log(\frac{a}{b}) = \log(a) - \log(b)$, $\log(ab) = \log(a) + \log(b)$ and $\log(\exp(x)) = x$ \\
		  
		  \begin{align*}
		  \ln p(\wvec) &= \underbrace{\frac{D}{2} \ln \alpha - \frac{D}{2} \ln (2 \pi)}_{\text{constant term}} + (- \frac{\alpha}{2}\wvec^T \wvec) \\
		  &= \frac{- \alpha}{2} \wvec^T \wvec + \mathcal{C}
		  \end{align*}
		  
		  Where $\mathcal{C}$ is a constant term. \\
  
  \item Write down an expression for the posterior over $\wvec$.  Remember this will involve applying Bayes rule to the prior, likelihood, and evidence.  The evidence will require an integral.  You do not need the analytic form for the evidence, but you need the correct variables and conditioning variables, e.g. something like $p(a|b,c)$ where you define $a$, $b$, and $c$.\\
	  \emph{Solution:} \\
		  According to Bayes rule we have:
		  
		  \begin{align*}
		  p(\wvec | \mathcal{D}) &= \frac{p(\wvec) p(\mathcal{D} | \wvec) }{p(\Data)} \\
		  \end{align*}
		  
		  From previous questions we got the likelihood and prior for the Bayes rule: \\
		  
		  \begin{align*}
		  p(\Data | \thetav) &= p(\Data | \wvec) \\
		  &= \mathcal{N}(\tvec | \Phimat \wvec, \frac{1}{\beta} \eye )
		  \end{align*}
		  
		  \begin{align*}
		  p(\wvec) &= \mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha}\eye)
		  \end{align*}
		  
		  We express the evidence by marginalizing. Since this is a continuous distribution, it requires an integral over $\wvec$ \\
		  
		  \begin{align*}
		  p(\Data) &= \int (p(\mathcal{D} | \wvec) p(\wvec) ) d\wvec\\
		  \end{align*}
		  
		  Substituting in Bayes Rule, we get: \\
		  
		  \begin{align*}
		  p(\wvec | \Data) &= \frac{\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye)}{\int {\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye) d\wvec}} \\ \\
		  &= \frac{\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye)}{\int p(\tvec | \Phimat, \wvec, \beta) p(\wvec | \alpha)d\wvec} \\ \\
		  &= \frac{\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye)}{p(t | \Phimat, \alpha, \beta)}
		  \end{align*}
  
  \item Compute the log-posterior, both for the a) and b) likelihood forms from above.  Collect everything that does not depend on $\wvec$ into a constant $C$.  What parts of the previous expression do not depend on $\wvec$?  Why is finding the MAP much simpler than finding the full posterior distribution? \\
	  \emph{Solution:} \\
		  \begin{align*}
		  \ln p(\wvec | \Data) &= \ln (\frac{p(\wvec) p(\mathcal{D} | \wvec) }{p(\Data)}) \\ \\
		  &= \ln p(\wvec) + \ln p(\mathcal{D} | \wvec) - \underbrace{\ln p(\Data)}_{\text{independent of \wvec}} \\ \\
		  &= - \frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} \sum_{n=1}^{N}(\tn - \wvec^T \phivecn)^{2} + \mathcal{C} \\ \\
		  &= - \frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} (\tvec - \Phimat \wvec)^{T} (\tvec - \Phimat \wvec) + \mathcal{C}
		  \end{align*}
		  
		  We can see that finding MAP is simpler because the evidence term (found in the full posterior distribution) becomes a constant term independent of $\wvec$, when finding its log. Furthermore, when finding the derivative of the log, the constant term goes to zero and it is discarded\\
  
  \item Solve for $\wvecMAP$ by a) taking the derivative of the log-posterior with respect to $\wvec$, b) setting it to 0, and c) solving for $\wvec$.  Do this for both forms of likelihood.\\
	  \emph{Solution:} \\
		  First we solve in the non-matrix form. Given that the log likelihood is: \\
		  \begin{align*}
		  \ln p(\wvec | \Data) &= - \frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} \sum_{n=1}^{N}(\tn- \wvec^T \phivecn)^{2} + \mathcal{C} 			\end{align*}
		  
		  We find its derivative and set it to $0$:
		  
		  \begin{align*}
		  \frac{\partial \ln p(\wvec | \Data)}{\partial \wvec} &= - \alpha \wvec - \beta \sum_{n=1}^{N} (\tn- \wvec^T \phivecn)(-\phivecn) = 0 
		  \end{align*}
		  
		  Solving for $\wvec$
		  
		  \begin{align*}
		  \alpha \wvec &= \beta \sum_{n=1}^{N} (\tn- \wvec^T \phivecn) \phivecn \\
		  \alpha \wvec &= \beta \sum_{n=1}^{N} \tn \phivecn - \underbrace{\wvec^T \phivecn}_{\text{scalar}} \phivecn \\
		  \alpha \wvec &= \beta \sum_{n=1}^{N} \tn \phivecn - \phivecn \underbrace{\wvec^T \phivecn}_{\text{same as } \phivecn^{T} \wvec} \\
		  (\alpha \eye + \beta \sum_{n=1}^{N} \phivecn\phivecn^T) \wvec &= \beta \sum_{n=1}^{N} \tn \phivecn 
		  \end{align*}
		  
		  \begin{align*}
		  \wvecMAP = (\alpha \eye + \beta \sum_{n=1}^{N} \phivecn\phivecn^T)^{-1} \beta \sum_{n=1}^{N} \tn \phivecn
		  \end{align*}
		  
		  Secondly, we solve in a matrix form. Again, given the log likelihood is: \\
		  
		  \begin{align*}
		  \ln{p(\wvec | \Data)} &= -\frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} (\tvec - \Phimat \wvec)^T (\tvec - \Phimat \wvec) + \mathcal{C} \\ \\
		  &= - \frac{\alpha}{2} \wvec^T\wvec - \frac{\beta}{2}\wvec^T \Phimat^T \Phimat \wvec + \beta \wvec^T \Phimat^T \tvec + \mathcal{D}
		  \end{align*}
		  
		  The derivative is given by: \\
		  
		  \begin{align*}
		  \frac{\partial \ln p(\wvec | \Data)}{\partial \wvec} &= - \alpha \wvec - \beta \Phimat^T \Phimat \wvec + \beta \Phimat^T \tvec = 0 
		  \end{align*}
		  
		  Solving for $\wvecMAP$:
		  
		  \begin{align*}
		  (\alpha \eye + \beta \Phimat^T \Phimat)\wvec = \beta \Phimat^T \tvec
		  \end{align*}
		  
		  \begin{align*}
		  \wvecMAP &= (\alpha \eye + \beta \Phimat^T \Phimat)^{-1} \beta \Phimat^T \tvec \\
		  &=(\beta (\frac{\alpha}{\beta} \eye + \Phimat^T\Phimat))^{-1} \beta \Phimat^T \tvec \\
		  &= \frac{1}{\beta}(\lambda \eye + \Phimat^T\Phimat)^{-1}\Phimat^T \tvec
		  \end{align*}
  
  \item {\bf BONUS}~~~Our prior for $\wvec$ assumes the same marginal distribution for each entry in $\wvec$, including that of the first basis function $\phivec_0 = 1$.  What is the role this basis function?  Why should we avoid placing the same penalty/prior for this basis?  Rewrite $p(\wvec)$ so that the first basis function has its own prior/penalty. \\
	  \emph{Solution:} \\
		  %The constant basis function acts as a bias or offset for the regression problem. If we use the same prior for this weight as for the others, we are assuming that the offset from the y-axis should somehow be penalized. This does not make too much sense a priori, so instead we use a different precision for this basis function, i.e. $\alpha_0 << \alpha$, while using $\alpha$ for all the others.
  
\end{enumerate}

\vspace{0.25in}
\section{Probability distributions, likelihoods, and estimators}
For these questions you will be working with different probability density functions listed in the table below.  The purpose of these questions is to practice working with a variety of PDFs and to make computing likelihoods, MLEs, etc. more natural. Note below the {\em indicator} notation $[x=0]$ (and $[x=1]$).  The square brackets evaluate to 1 if the argument is true, and 0 otherwise.  E.g. if $x$ is $1$, the $[x=0] = 0$ and $[x=1] = 1$ (here $[x=0]$ is lazy notation; in Python you would write $x==0$, for example).  We will use the notation a lot, both below and when we learn about classification.
%
\begin{table}[h!]
  \centering
\begin{tabular}{|l|c|c|c|}
  \hline
  Distribution & $p(x | \theta )$ &   Range of x & Range of $\theta$ \\ \hline\hline
  Bernouilli & $\theta^{[x=1]}(1-\theta)^{[x=0]}$ & $x \in \{0,1\}$ & $0 \leq \theta \leq 1$ \\\hline
  Beta & $\frac{\Gamma\lp\theta_1+\theta_0\rp}{\Gamma\lp\theta_1\rp\Gamma\lp\theta_0\rp} x^{\theta_1-1}(1-x)^{\theta_0-1}$ & $0 \leq x \leq 1$ & $\theta_1 > 0, \theta_0 > 0$ \\ \hline
  %
  Poisson & $\frac{\theta^x}{x!}e^{-\theta}$& $x \in \{0,1,2,\ldots\}$ & $\theta > 0$ \\ \hline
  %
  Gamma & $\frac{\theta_1^{\theta_0}}{\Gamma\lp\theta_0\rp} x^{\theta_0-1}e^{-\theta_1 x}$& $x \geq 0$ & $\theta_1 \geq 0$, $\theta_0 \geq 0$ \\ \hline
  %
  Gaussian & $\frac{1}{\sqrt{2\pi\theta_1}} e^{-\frac{1}{2}\lp \frac{x-\theta_0}{\theta_1}\rp^2}$ & $-\infty < x < \infty$ &  $-\infty < \theta_0 < \infty$,  $\theta_1>1$  \\ \hline
\end{tabular}
\end{table}

\subsection{Question}
For each of the probability distributions above, write down their normalizing constants.  Remember that $\int p(x|\theta) d x = 1$ for continuous $x$ and $\sum_x p(x| \theta) = 1$ for discrete $x$.  \\
	\emph{Solution:} \\
		The normalizing constants are those terms which do not depend on $x$, but that help to mantain the sum of the probabilities for all $x$ equal to $0$. Hereby we state the constants for the distributions given: \\
		
		\begin{enumerate}
			\item{Bernouilli} \\
			There is no normalizing constant in this case, since $X \in \{1,0\}$ and its probabilities are complementary.
			
			\item{Beta}
			\begin{align*}
			\frac{\Gamma (\theta_1 + \theta_0)}{\Gamma (\theta_1) \Gamma (\theta_0)}
			\end{align*}
			
			\item{Poisson}
			\begin{align*}
			\exp^{\theta}
			\end{align*}
			
			\item{Gamma}
			\begin{align*}
			\frac{\theta_{1}^{\theta_{0}}}{\Gamma (\theta_{0})}
			\end{align*}
			
			\item{Gaussian}
			\begin{align*}
			\frac{1}{\sqrt[]{2\pi \theta_{1}}}
			\end{align*}
			
		\end{enumerate}

\subsection{Question}
You live in Amsterdam and find that it rains quite a lot.  You want to estimate the probability that it will rain any given day of the year.  Every month for a year you count the number of days with rain, and you get the following (from January to December): 22,19,16,16,14,14,17,18,19,20,21,21 (for a grand total of 217 days with rain).\footnote{Source: \url{http://www.amsterdam.climatemps.com/}.}  Let $r_t$ be an observation for day $t$ in the year; $r_t=1$ means there was some rain on day $t$, $r_t=0$ means there was no rain.  We want to estimate the parameter $\rho$, the probability of rain on any day of the year.  We assume a Bernouilli distribution for the observations $\{r_t\}_{t=1}^{365}$, that is $p( r_t | \rho ) = \text{Bernouilli}(r_t | \rho)$.  To answer these questions, the number of days of rain per month is not important, only the total for the year is relevant.  With this information, answer the following questions:
\begin{enumerate}
  \item What is the likelihood for a single observation?  For the entire set of observations? \\
	  \emph{Solution:} \\
		  The likelihood of a single observation is: \\
		  \begin{align*}
		  p(r_t | \rho) = \rho^{r_t} (1 - \rho)^{1 - r_t}
		  \end{align*}
		  
		  Then, we find the probability of the whole set, representing it as the product of single observations. Using matrix notation we get: \\
		  
		  \begin{align*}
		  p(\rvec | \rho) &= \prod_{t = 1}^{T} \rho^{r_t} (1 - \rho)^{1 - r_t} \\
		  &= \rho^{\sum_{t = 1}^{T} r_t} (1 - \rho)^{\sum_{t = 1}^{T} 1 - r_t} \\
		  &= \rho^{n_1} (1 - \rho)^{n_0}
		  \end{align*}
		  
		  Where: \\
		  
		  \begin{align*}
		  \underbrace{n_1}_{\text{days with rain}} &= \sum_{t = 1}^{T} r_t \\ 
		  \underbrace{n_0}_{\text{days without rain}} &= \sum_{t = 1}^{T} 1 - r_t
		  \end{align*}
  
  \item Write the log-likelihood for the entire set of observations.  \\
	  \emph{Solution:} \\
		  Using the logarithm arithmetic rules, we get: \\
		  
		  \begin{align*}
		  \ln p(\rvec | \rho) &= \ln \rho^{n_1} (1 - \rho)^{n_0} \\ \\
		  &= ln \rho^{n_1} + \ln (1 - \rho)^{n_0} \\ \\
		  &= n_1 \ln \rho + n_0 \ln (1 - \rho)
		  \end{align*}
  
  \item Solve for the MLE of $\rho$.  Do it in general (with symbols for counts $n_0$, $n_1$ for days without and with rain) and for this specific case (plug-in the numbers).\\
	  \emph{Solution:} \\
		  In order to compute the MLE, first we find the derivative of the loglikelihood, and set it to $0$. \\
		  
		  \begin{align*}
		  \frac{\partial \ln p(\rvec | \rho)}{\partial \rho} &= \frac{n_1}{\rho} + \frac{n_0}{1 - \rho} (-1) = 0 \\
		  \end{align*}
		  
		  Then, we find the value of $\rho$
		  
		  \begin{align*}
		  \frac{n_1}{\rho} &= \frac{n_0}{1 - \rho} \\ \\
		  n_1 - n_1 \rho &= n_0 \rho \\ \\
		  \rho &= \frac{n_1}{N}
		  \end{align*}
		  
		  Plugin in the number, we have: 
		  
		  \begin{align*}
		  \rho &= 217 / 365
		  \end{align*}
  
  \item Assume a Beta prior for $\rho$ with parameters $a$ and $b$.  What is the MAP for $\rho$? \\
	  \emph{Solution:} \\
		  We follow the same procedure, but this time for the posterior, given by: \\        
		  
		  \begin{align*}
		  f = \ln p(\rho | \rvec) &\propto \ln p(\rho | \rvec) + \ln p(\rho) \\
		  &= n_1 \ln \rho + n_0 \ln(1 - \rho) + (a - 1) \ln \rho + (b - 1) \ln(1 - \rho)
		  \end{align*}
		  
		  We first find the derivative and set it to $0$.
		  
		  \begin{align*}
		  \frac{\partial f}{\partial{\rho}} &= \frac{n_1}{\rho}-\frac{n_0}{1-\rho}+\frac{a -1}{\rho}-\frac{b-1}{1-\rho} = 0\\
		  \end{align*}  
		  
		  Then we solve for $\rho$
		  
		  \begin{align*}
		  &\rho= \frac{n_1 + a -1}{N + a +b -2}
		  \end{align*} 
  
  
  \item Write the form of the posterior distribution for $\rho$?  You do not need to solve it analytically. \\
	  \emph{Solution:} \\
		  Using Bayes Theorem, we have: \\
		  \begin{align*}
		  p(\rho\vert\textbf{r}) &= \frac{p(\textbf{r}\vert\rho)p(\rho)}{p(\textbf{r})} \\ \\
		  \end{align*} 
		  
		  Substituting with the specifics of this problem, we get: \\ 
		  
		  \begin{align*}
		  p(\rho\vert\textbf{r}) &= \frac{\rho^{n_1+a-1}(1-\rho)^{n_0+b-1}}{\int\rho^{n_1+a-1}(1-\rho)^{n_0+b-1}dp} \\ \\
		  &=\frac{\Gamma(\mathcal{N}+a+b)}{\Gamma(n_1+a)\Gamma(n_0+b)}\rho^{n_1+a-1}(1-\rho)^{n_0+b-1} \\ \\
		  &= \mathcal{B}(\rho \vert a +n_1,b + n_0)
		  \end{align*} 
  
  
  \item (Optional) Solve for the posterior distribution analytically.  Hint: it is a Beta distribution. \\
	  \emph{Solution:} \\
  
  
\end{enumerate}

\subsection{Question}
You work in the staffing department of a maternity hospital and part of your job is to determine the staffing requirements during the night shift at your hospital.  This might mean the number of doctors and nurses at the hospital and the number of doctors on call (if there are more than the average number of deliveries).  Your goal is to determine the distribution over the number of deliveries during the night shift $d_t \in \{0,1,2,\ldots\}$ ($d$ for delivery count, $t$ for time, the index of the night).  With this you can compute the mean, the probability of more than $5$ deliveries, etc.  You collect data for two weeks, i.e. $d_1, \ldots, d_{14} = 4, 7, 3, 0, 2, 2, 1, 5, 4, 4, 3, 3, 2, 3$.  You assume the observations are explained by a Poisson distribution with parameter $\lambda$ over the discrete delivery counts. With this information, answer the following questions:
%
\begin{enumerate}
  \item What is the likelihood for a single observation?  For the entire set of observations? \\
	  \emph{Solution:} \\
		  The likelihood of a single observation is: \\
		  \begin{align*}
		  &p(d_t\vert \lambda) = \frac{\lambda^{d_t}}{d_t!}\exp(-\lambda) \\
		  \end{align*}
		  
		  Then, we find the probability of the whole set, representing it as the product of single observations. Using matrix notation we get: \\
		  \begin{align*}      
		  p(\textbf{d} \vert \lambda)&=\prod_{t=1}^{T} \frac{\lambda^{d_t}}{d_t!}\exp(-\lambda) \\
		  &=\frac{\lambda^{\sum_{t=1}^{T}d_t}}{\prod_{t=1}^{T}d_t!}\exp(-T\lambda) \\
		  &=\frac{\lambda^{n}}{\prod_{t=1}^{T}d_t!}\exp(-T\lambda)
		  \end{align*} 
  
  \item Write the log-likelihood for the entire set of observations. \\
	  \emph{Solution:} \\
		  Using the logarithm arithmetic rules, we get: \\        
		  
		  \begin{align*}
		  \ln p(\textbf{d}\vert\lambda) = n\ln\lambda - T \lambda - \sum_{t=1}^{T}\ln(d_t!)
		  \end{align*}
  
   
  \item Solve for the MLE of $\lambda$.  Do it in general and for this specific case (plug-in the numbers). \\
	  \emph{Solution:} \\
		  \begin{align*}
		  	f &= \ln p(\textbf{d} \vert \lambda)\\ 
		  	&= n \ln\lambda - T\lambda- \sum_{t=1}^{T}\ln(d_t!) \\
		  	\frac{\partial f}{\partial \lambda} &= \frac{n}{\lambda} - T = 0 \\
		  	\lambda &= \frac{n}{T} = \frac{43}{14}
		  \end{align*}
  
  \item Assume a Gamma prior for $\lambda$ with parameters $a$ and $b$.  What is the MAP estimate of $\lambda$? \\
	  \emph{Solution:} \\
		  \begin{align*}
		  	f &= \ln p(\textbf{d} \vert \lambda)+\ln p(\lambda) \\
		  	&= n \ln \lambda - T\lambda - \sum_{t=1}^{T}d_t! + (a-1)\ln\lambda-b\lambda + C \\ 
		  	\frac{\partial f}{\partial \lambda} &= \frac{n}{\lambda} - T + \frac{(a - 1)}{\lambda} - b = 0 \\
		  	&\lambda = \frac{n + a -1}{T + b}\\
		  \end{align*}
  
  \item Write the form of the posterior distribution for $\lambda$? (You do not need to solve it analytically) \\
	  \emph{Solution:} \\
		  \begin{align*}
		  p(\lambda \vert \textbf{d}) &= \frac{p(\textbf{d}\vert \lambda)p(\lambda)}{\int p(\textbf{d}\vert \lambda)p(\lambda)d\lambda} \\
		  &= \frac{\frac{\lambda^n}{\prod_{t=1}^{T}d_t!} \exp(-T\lambda)\frac{b^a}{\Gamma(a)}\lambda^{a-1} \exp(-b\lambda)}{\int \frac{\lambda^n}{\prod_{t=1}^{T}d_t!}\exp(-T\lambda)\frac{b^a}{\Gamma(a)}\lambda^{a-1} \exp(-b\lambda)d\lambda}\\ 
		  &=\frac{\lambda^{n+a-1}\exp(-(T + b)\lambda)}{\int\lambda^{n+a-1}\exp(-(T + b)\lambda)d\lambda} \\
		  &=\frac{(T+b)^{n+a}}{\Gamma(n+a)}\lambda^{n+a-1}\exp(-(T + b)\lambda)\\
		  &= \mathcal{G}(\lambda \vert a + n, b + T)
		  \end{align*}
  
  
  \item (Optional) Solve for the posterior distribution analytically.  Hint: it is a Gamma distribution. \\
	  \emph{Solution:} \\
  
\end{enumerate}

\subsection{Question}
You have developed a blood test aimed at detecting a disease $d \in \{0,1\}$ (disease is absent ($d=0$) or present ($d=1$)).  The test measures the level of a specific indicator of the disease, that is it returns a real valued number relative to some baseline (so the levels can be both negative and positive -- anywhere along the real line).  Two models of the population are built: one for the patients with the disease, and another for the general population.  Measurements tend to have a Gaussian shape, and we therefore model the entire population as a mixture of two Gaussians.  That is, $p( l ) = p(d=0) p( l | d=0 ) + p(d=1) p( l | d=1 )$, where $p(d)$ is the prior distribution of patients with and without the disease in the general population and $p(l|d)$  are conditional Gaussian distributions, one for the patients with disease, and one for those without.  Note: with this question and the previous two, we are simply applying rules of probability (with some algebra) to get the form of the posterior distribution; however, in this problem we are also classifying (since our target is the discrete label $d$).

Assume we know $p(d=0) = \pi_0 = 0.999$ and $p(d=1) = \pi_1 = 0.001$ from previous experience.  We do not know the parameters $\mu_0, \sigma_0^2$ (the mean and variance of the disease-free population) nor $\mu_1, \sigma_1^2$ (for the disease population).  We measure levels $\{ l_n\}_{n=1}^N$ for N people, and we know that $n\in\{D_0\}$ are the indices for the disease free patients and $n\in\{D_1\}$ are the indices for the patients with the disease (i.e. $D_0$ and $D_1$ are non-intersecting sets of indices from $1$ to $N$).  With this information, answer the following questions:

\begin{enumerate}
  \item Write down the likelihood of the observations as a product over $N$ level recordings.  Hint: use indicator notation (like in the Bernouilli distribution) to distinguish between $d_n=0$ and $d_n=1$ in the likelihood. \\
  \emph{Solution:} \\
	  \begin{align*}
	  	p(\dvec, \lvec | \thetavec) &= \prod_{n = 1}^{N} (\pi_1 \mathcal{N} (l_{n} | \mu_{1}, \sigma_{1}^{2}) )^{d_n} (\pi_0 \mathcal{N} (l_{n} | \mu_{0}, \sigma_{0}^{2})) ^{1 - d_n} 
	  \end{align*}
  
  \item Write down the likelihood as a product over the likelihoods for $\{D_0\}$ and $\{D_1\}$. \\
	  \emph{Solution:} \\
		  \begin{align*}
		  	p(\dvec, \lvec | \thetavec) 
		  	&= ( \prod_{n \in D_1} \pi_{1} \mathcal{N}(l_n | \mu_{1} , \sigma_{1}^{2})) ( \prod_{n \in D_0} \pi_{0} \mathcal{N}(l_n | \mu_{0} , \sigma_{0}^{2}))
		  \end{align*}
  
  \item Compute the log-likelihood. \\
	  \emph{Solution:} \\
		  \begin{align*}
		  	\ln p(\dvec, \lvec | \thetavec) &= \sum_{n \in D_{1}}(\ln \pi_{1} + \ln \mathcal{N}(l_{n} | \mu_{1}, \sigma_{1}^{2}) ) + \sum_{n \in D_{0}} (\ln \pi_{0} + \ln \mathcal{N}(l_{n} | \mu_{0}, \sigma_{0}^{2})) \\
		  	&= \sum_{n \in D_{1}}(\ln \pi_{1} + \frac{1}{2} \ln (2 \pi) - \frac{1}{2} \ln (\sigma_{1}^{2}) - \frac{1}{2} \frac{(l_{n} - \mu_{1})^{2}}{\sigma_{1}^{2}}) \\
		  	&+ \sum_{n \in D_{0}}  (\ln \pi_{0} + \frac{1}{2} \ln (2 \pi) - \frac{1}{2} \ln (\sigma_{0}^{2}) - \frac{1}{2} \frac{(l_{n} - \mu_{1})^{2}}{\sigma_{0}^{2}})
		  \end{align*}
  
  \item Find the MLE for $\mu_0$ and $\sigma_0^2$.  Assume we can do the same for $\mu_1$ and $\sigma_1^2$ \\
	  \emph{Solution:} \\
		  \begin{align*}
		  	\frac{\partial \ln p(\dvec, \lvec | \thetavec)}{\partial \mu_{0}} &= \sum_{n \in D_{0}} (- \frac{(l_{n} - \mu_{0})}{\sigma_{0}^{2}} (-1)) = 0 \\
		  	0 &= \sum_{n \in D_{0}} (l_{n} - \mu_{0}) \\
		  	0 &= \sum_{n \in D_{0}} l_{n} - N_{0} \mu_{0} \\
		  	\mu_{0} &= \frac{1}{N_{0}} \sum_{n \in D_{0}} l_{n}
		  \end{align*}
		  
		  \begin{align*}
		  	\frac{\partial \ln p(\dvec, \lvec | \thetavec)}{\partial \sigma_{0}^{2}} &= \sum_{n \in D_{0}} (- \frac{1}{2 \sigma_{0}^{2}} - \frac{1}{2} \frac{(l_{n} - \mu_{0})^{2}}{(\sigma_{0}^{2})^{2}}(-1)) = 0 \\
		  	\frac{N_{0}}{2 \sigma_{0}^{2}} &= \frac{1}{2} \frac{\sum_{n \in D_{0}} (l_{n} - \mu_{0})^{2}}{(\sigma_{0}^{2}) ^{2}} \\
		  	\sigma_{0}^{2} &= \frac{1}{N_{0}} \sum_{n \in D_{0}} (l_{n} - \mu_{0})^{2}
		  \end{align*}
  
  \item We now have our models.  To make a prediction, solve for $p( d=1 | l_{\star})$, where $l_{\star}$ is a level recorded for a new patient.  Hint: use Bayes theorem. \\
	  \emph{Solution:} \\
		  \begin{align*}
		  	p(d = 1 | l_{\star}) &= \frac{\pi_1 \mathcal{N} (l_{\star} | \mu_{1}, \sigma_{1}^{2})}{\pi_1 \mathcal{N} (l_{\star} | \mu_{1}, \sigma_{1}^{2}) + \pi_0 \mathcal{N} (l_{\star} | \mu_{0}, \sigma_{0}^{2})} \\
		  	&= \frac{1}{1 + \frac{\pi_0 \mathcal{N} (l_{\star} | \mu_{0}, \sigma_{0}^{2})}{\pi_1 \mathcal{N} (l_{\star} | \mu_{1}, \sigma_{1}^{2})}} \\
		  	&= \frac{1}{1 + \exp^{-a(l_{\star})}} \\
		  	a(l_{\star}) &= \ln (\frac{\pi_1 \mathcal{N} (l_{\star} | \mu_{1}, \sigma_{1}^{2})}{\pi_0 \mathcal{N} (l_{\star} | \mu_{0}, \sigma_{0}^{2})})
		  \end{align*}
  
  
  \item Reduce your solution to have the form of a sigmoid, i.e. 
      \begin{equation*}
         p(d=1|l_{\star}) = \frac{1}{1+e^{-a(l_{\star})}}.
      \end{equation*}
      \\
	      \emph{Solution:} \\
		      \begin{align*}
		      	p(d = 1 | l_{\star}) = \frac{1}{1 + \exp ^{-a(l_{\star})}} 
		      	&= \frac{1}{1 + \frac{\pi_0 \mathcal{N} (l_{\star} | \mu_{0}, \sigma_{0}^{2})}{\pi_1 \mathcal{N} (l_{\star} | \mu_{1}, \sigma_{1}^{2})}} \\
		      \end{align*}
		      
		  Since we know the Normal distribution has a sigmoid form, this last solution has the desired form. 
   
  
\end{enumerate}

\end{document}