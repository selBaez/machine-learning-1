\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage[compact,explicit]{titlesec}
\usepackage{enumitem}
\usepackage{verbatim}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\loglikelihood}{loglikelihood}
\newcommand{\bishop}[1]{Bishop #1}
\newcommand{\newword}[1]{{\bf #1}}
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Model}{\mathcal{M}}
\newcommand{\DataTrain}{\mathcal{D}_{{\text train}}}
\newcommand{\DataTest}{\mathcal{D}_{{\text test}}}
\newcommand{\N}{N}
\newcommand{\Ntrain}{\N_{train}}
\newcommand{\Ntest}{\N_{test}}
\newcommand{\DataSize}{\N}
\newcommand{\DataIndex}{n}
\newcommand{\cind}{t}
\newcommand{\pind}{\star}

\newcommand{\eye}{{\bf I}}

\newcommand{\Dim}{D}
\newcommand{\DimIndex}{d}

\newcommand{\DimOut}{K}
\newcommand{\DimOutIndex}{k}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\ED}{\mathbb{E}_{\Data}}
\DeclareMathOperator*{\V}{\mathbb{V}}
\DeclareMathOperator*{\LL}{L}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\trace{tr}
\DeclareMathOperator\median{median}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\expectation}{\E}
\newcommand{\expectationdata}{\ED}
\newcommand{\variance}{\V}
\newcommand{\loss}{\LL}
\newcommand{\ascalar}{a}
\newcommand{\xscalar}{x}
\newcommand{\xvec}{{\bf \xscalar}}
\newcommand{\xvectest}{\xvec_{\star}}
\newcommand{\Xmat}{{\bf \MakeUppercase\xscalar}}
\newcommand{\tscalar}{t}
\newcommand{\ttest}{\tscalar_{\star}}
\newcommand{\tvec}{{\bf \tscalar}}
\newcommand{\Tmat}{{\bf \MakeUppercase\tscalar}}
\newcommand{\yscalar}{y}
\newcommand{\yvec}{{\bf \yscalar}}
\newcommand{\Ymat}{{\bf \MakeUppercase\yscalar}}
\newcommand{\wscalar}{w}
\newcommand{\wvec}{{\bf \wscalar}}
\newcommand{\wvecML}{\wvec_{\text{MLE}}}
\newcommand{\wvecMAP}{\wvec_{\text{MAP}}}
\newcommand{\wvecs}{\wvec^{(s)}}
\newcommand{\Wmat}{{\bf \MakeUppercase\wscalar}}
\newcommand{\wbias}{\wscalar_0}
\newcommand{\xn}{\xscalar_{\DataIndex}}
\newcommand{\xvecn}{\xvec_{\DataIndex}}
\newcommand{\tvecn}{\tvec_{\DataIndex}}
\newcommand{\tn}{\tscalar_{\DataIndex}}
\newcommand{\ti}{\tscalar_{i}}
\newcommand{\yvecn}{\yvec_{\DataIndex}}
\newcommand{\yn}{\yscalar_{\DataIndex}}
\newcommand{\yfunc}{\yscalar}
\newcommand{\yfunctest}{\yfunc_{\star}}

\newcommand{\zerovec}{ {\bf 0}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\muvecN}{\muvec_{\N}}
\newcommand{\munotvec}{\boldsymbol{\mu}_0}
\newcommand{\mnot}{{\bf m_0}}
\newcommand{\mN}{{\bf m}_{\N}}
\newcommand{\mNplus}{{\bf m}_{\N+1}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\thetav}{\thetavec}
\newcommand{\phivec}{\boldsymbol{\phi}}
\newcommand{\phivectest}{\phivec_{\star}}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\phivv}{\phivec}
\newcommand{\phivecn}{\phivec_{\DataIndex}}
\newcommand{\phiveci}{\phivec_{i}}
\newcommand{\Sigmamat}{\boldsymbol{\Sigma}}
\newcommand{\SigmamatN}{\Sigmamat_N}
\newcommand{\Sigmamatnot}{\Sigmamat_0}
\newcommand{\SigmamatInv}{\Sigmamat^{-1}}
\newcommand{\Smat}{{\bf S}}
\newcommand{\SmatN}{\Smat_{\N}}
\newcommand{\Smatnot}{\Smat_0}
\newcommand{\SmatInv}{\Smat^{-1}}
\newcommand{\SmatNplus}{\Smat_{\N+1}}
\newcommand{\SmatNplusinv}{\Smat_{\N+1}^{-1}}

\newcommand{\betatest}{\beta_{\star}}
\newcommand{\Amat}{{\bf A}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}{{\bf E}}
\newcommand{\Fmat}{{\bf F}}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}{{\bf J}}
\newcommand{\Kmat}{{\bf K}}
\newcommand{\Lmat}{{\bf L}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}{{\bf N}}
\newcommand{\Omat}{{\bf O}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}{{\bf Q}}
\newcommand{\Rmat}{{\bf R}}
%\newcommand{\Smat}{{\bf S}}
%\newcommand{\Tmat}{{\bf T}}
\newcommand{\Umat}{{\bf U}}
\newcommand{\Vmat}{{\bf V}}
%\newcommand{\Wmat}{{\bf W}}
%\newcommand{\Xmat}{{\bf X}}
%\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}



\newcommand{\rvec}{{\bf r}}
\newcommand{\dvec}{{\bf d}}
\newcommand{\lvec}{{\bf l}}
\newcommand{\mvec}{{\bf m}}
\newcommand{\uvec}{{\bf u}}
\newcommand{\vvec}{{\bf v}}

\newcommand{\xvecmean}{\bar{\xvec}}
\newcommand{\xvecnest}{\tilde{\xvec}_n}
\newcommand{\xvecestn}{\xvecnest}
\newcommand{\class}{\mathcal{C}}
\newcommand{\sigmoid}{\sigma}

\newcommand{\ans}[1]{ 
	\begin{center}
		% \fbox{
		\begin{minipage}{.8\textwidth}
			{\sf#1}
		\end{minipage}
		% }
	\end{center} 
}

\title{Machine Learning 1 - Homework 3}
\author{Selene Baez Santamaria}

%\author{Monday, September 12, 2016 \\ Deadline: Wednesday, September 21, 2016, 23:59}
\date{}
\begin{document}
	\maketitle
	\titleformat{\section}[runin]{\large\bfseries}{}{0pt}{\thesection\quad\underline{#1}\vspace{0.1in}\newline}
	\titleformat{\subsection}[runin]{\normalsize\bfseries}{}{0pt}{#1 \thesubsection\newline}
	
	\vspace{0.25in}
	\section{Naive Bayes Spam Classification}
	Answer the following:
	\begin{enumerate}
		\item Write down the likelihood for the general two class naive Bayes classifier. \\
		\emph{Solution:} \\
		
		\begin{align*}
			p(\textbf{T},\textbf{X} | \boldsymbol{\theta}) &= \prod_{n=1}^{N}(\pi_1 \prod_{d=1}^{D}p(x_nd | \theta_1))^{1 - t_n} (\pi_2 \prod_{d=1}^{D}p(x_nd | \theta_2))^{t_n} \\
		\end{align*}		  
		
		\item Write down the likelihood for the Poisson model. \\
		\emph{Solution:} \\
		We substitute the general probability expression for the Poisson distribution:
		
		\begin{align*}
			p(\textbf{T},\textbf{X} | \boldsymbol{\theta}) &= \prod_{n=1}^{N}(\pi_1 \prod_{d=1}^{D}\frac{\lambda_{d1}^{x_nd}}{x_nd!}\exp(-\lambda_{d1}))^{1 - t_n} (\pi_2 \prod_{d=1}^{D}\frac{\lambda_{d2}^{x_nd}}{x_nd!}\exp(-\lambda_{d2}))^{t_n}\\
		\end{align*}
		
		\item Write down the log-likelihood for the Poisson model. \\
		\emph{Solution:} \\
		Taking the logarithm of the likelihood, we get:
		
		\begin{align*}
			\ln p(\textbf{T},\textbf{X} | \boldsymbol{\theta}) &= \sum_{n \in C_1}^{N}(\ln\pi_1 + \sum_{d = 1}^{D}x_{nd}\ln\lambda_{d1} - \ln(x_{nd}!) - \lambda_{d1}) \\
			&+ \sum_{n \in C_2}^{N}(\ln\pi_2 + \sum_{d=1}^{D} x_{nd}\ln\lambda_{d2} - \ln(x_{nd}!) - \lambda_{d2}) \\
		\end{align*}
		
		\item Solve for the MLE estimators for $\lambda_{dk}$ \\
		\emph{Solution:} \\
		The MLE estimator is found by finding the partial derivative of the log-likelihood over $\lambda_{dk}$ and setting it to $0$. In this case it is simple because most of the terms are independent from $\lambda_{dk}$
		
		\begin{align*}
			\frac{\partial \ln p(\textbf{T},\textbf{X} | \boldsymbol{\theta})}{\partial \lambda_{dk}} &= \sum_{n \in C_k}^{N}(\frac{x_{nd}}{\lambda_{dk}} - 1) = 0
		\end{align*}
		
		Solving for $\lambda_{dk}$, we get:
		
		\begin{align*}
			\sum_{n \in C_k}^{N} \frac{x_{nd}}{\lambda_{dk}} &= \sum_{n \in C_k}^{N} 1 \\
			\frac{1}{\lambda_{dk}} \sum_{n \in C_k}^{N}x_{nd} &= N_k \\
			\lambda_{dk} &= \frac{1}{N_k} \sum_{n \in C_k}^{N} x_{nd}\\
		\end{align*}
		
		$\lambda_{dk}$ represents the average number of word \textit{d} per class \textit{k}.      
		
		\item Write $p(\class_{1} | \xvec)$ for the general two class naive Bayes classifier.\\
		\emph{Solution:} \\
		\begin{align*}
			p(\class_{1} | \xvec) &= \frac{\pi_1 \prod_{d = 1}^{D}p(x_{nd}\theta_1)}{\pi_1 \prod_{d = 1}^{D}p(x_{nd}\theta_1) + \pi_2 \prod_{d = 1}^{D}p(x_{nd}\theta_2)}
		\end{align*}
		
		\item Write $p(\class_{1} | \xvec)$ for the Poisson model. \\
		\emph{Solution:} \\
		Again, substituting the Poisson distribution model:
		
		\begin{align*}
			p(\class_{1} | \xvec) &= \frac{\pi_1 \prod_{d = 1}^{D} \frac{\lambda_{d1}^{x_{nd}}}{x_nd!} \exp(-\lambda_{d1})} {\pi_1 \prod_{d = 1}^{D} \frac{\lambda_{d1}^{x_{nd}}}{x_nd!} \exp(-\lambda_{d1}) + \pi_2 \prod_{d=1}^{D} \frac{\lambda_{d2}^{x_{nd}}}{x_nd!} \exp(-\lambda_{d2})} \\\\
			&= \frac{ \lp \prod_{d=1}^{D}\frac{1}{x_nd!} \rp\pi_1 \prod_{d = 1}^{D} \lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1})} {\lp \prod_{d=1}^{D}\frac{1}{x_nd!} \rp \pi_1 \prod_{d = 1}^{D}\lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1}) + \pi_2 \prod_{d=1}^{D}\lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2})} \\\\
			&= \frac{\pi_1 \prod_{d = 1}^{D} \lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1})} {\pi_1 \prod_{d = 1}^{D}\lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1}) + \pi_2 \prod_{d=1}^{D}\lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2})}\\
		\end{align*}	
		
		\item Rewrite $p(\class_{1} | \xvec)$ as a sigmoid $\sigmoid(a) = \frac{1}{1 + \exp(-a)}$ ; solve for $a$ for the Poisson model. \\
		\emph{Solution:} \\
		
		\begin{align*}
			p(\class_{1} | \xvec) &= \frac{1}{1 + \exp(-\alpha)}\\
			&= \frac{1} {1 + \frac{\pi_2 \prod_{d=1}^{D}\lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2})}{\pi_1 \prod_{d = 1}^{D}\lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1})}} \\
		\end{align*}
		
		Solving for $\alpha$
		
		\begin{align*}
			\exp(-\alpha) &= \frac{\pi_2 \prod_{d=1}^{D}\lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2})}{\pi_1 \prod_{d = 1}^{D}\lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1})}\\\\
			\ln \lp \exp(-\alpha) \rp &= \ln \lp \frac{\pi_2 \prod_{d=1}^{D}\lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2})}{\pi_1 \prod_{d = 1}^{D}\lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1})} \rp \\\\
			-\alpha &= \ln \lp \pi_2 \prod_{d=1}^{D}\lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2}) \rp - \ln \lp \pi_1 \prod_{d = 1}^{D}\lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1}) \rp \\\\
			\alpha &= \ln \lp \pi_1 \rp + \sum_{d = 1}^{D} \ln \lp \lambda_{d1}^{x_{nd}} \exp(-\lambda_{d1}) \rp - \ln \lp \pi_2 \rp - \sum_{d=1}^{D}\ln \lp \lambda_{d2}^{x_{nd}} \exp(-\lambda_{d2}) \rp  \\\\
			\alpha &= \ln \lp \pi_1 \rp + \sum_{d = 1}^{D} \ln \lp \lambda_{d1}^{x_{nd}} \rp + \sum_{d = 1}^{D} -\lambda_{d1} - \ln \lp \pi_2 \rp - \sum_{d=1}^{D} \ln \lp \lambda_{d2}^{x_{nd}} \rp - \sum_{d=1}^{D} -\lambda_{d2}  \\\\
			\alpha &= \ln \frac{\pi_1}{\pi_2} + \sum_{d=1}^{D} x_{nd} \ln\frac{\lambda_{d1}}{\lambda_{d2}} - \sum_{d=1}^{D}(\lambda_{d1} - \lambda_{d2}) \\
		\end{align*}
		
		\item Assume $a = \wvec^{T}x + w_{0}$; solve for $\wvec$ and $w0$. \\
		\emph{Solution:} \\
		Gathering terms dependent on $x$, and rearranging:
		
		\begin{align*}
			\alpha &= \underbrace{\sum_{d=1}^{D} x_{nd} \ln\frac{\lambda_{d1}}{\lambda_{d2}}}_{w^{T} x_n}  + \underbrace{\ln \frac{\pi_1}{\pi_2} - \sum_{d=1}^{D}(\lambda_{d1} - \lambda_{d2})}_{w_0} \\
		\end{align*}
		
		Then: 
		\begin{align*}
			w_d = \ln\frac{\lambda_{d1}}{\lambda_{d2}}
		\end{align*}
		
		
		\item Is the decision boundary a linear function of $\xvec$? Why? \\
		\emph{Solution:} \\
		Since $w_0$ is a constant for all x, then the decision boundary is linear
		
	\end{enumerate}
	
	\vspace{0.25in}
	\section{Multi-class Logistic Regression}
	For $K > 2$ the posterior probabilities take a generalized form of the sigmoid called the softmax:
	\begin{align*}
		y_{k}(\phivec) = p(\class_{k} | \phivec ) &=
		\frac{\exp(a_{k})}{\sum_i \exp(a_{i})}
	\end{align*}
	where $a_{k} = \wvec_{k}^{T} \phivec$
	
	Answer the following:
	\begin{enumerate}
		\item Derive $\frac{\partial_{y_{k}}}{\partial\wvec_j}$. Bishop uses an indicator function $\eye_{kj}$, entries of the identity matrix; previously we used $[k = j]$ |they are the same thing. \\
		\emph{Solution:} \\
		
		\begin{align*}
			\frac{\partial y_k(\phivec)}{\partial \wvec_j} &= \frac{\partial}{\partial \wvec_j} \lp \frac{\exp(a_k)}{\sum_i \exp(a_i)} \rp \\\\
			&= \frac{\exp(a_k)}{\sum_i \exp(a_i)} \frac{\partial a_k}{\partial \wvec_j} - \frac{\exp(a_k) \exp(a_j)}{\lp \sum_i \exp(a_i) \rp^2}  \frac{\partial a_k}{\partial \wvec_j} \\\\
			&= [k = j] \frac{\exp(a_k)}{\sum_i \exp(a_i)} \phivec - \frac{\exp(a_k)}{\sum_i \exp(a_i)} \frac{\exp(a_j)}{\sum_i \exp(a_i)} \phivec \\\\
			&= \frac{\exp(a_k)}{\sum_i \exp(a_i)} \lp [k = j] - \frac{\exp(a_j)}{\sum_i \exp(a_i)} \rp \phivec \\\\
			&= y_k(\phivec) \lp \eye_{kj} - y_j(\phivec) \rp \phivec
		\end{align*}		  
		
		\item Write down the likelihood as a product over $N$ and $K$ then write down the log-likelihood. Use the entries of $\textbf{T}$ as selectors of the correct class. \\
		\emph{Solution:} \\
		The likelihood is written as:
		\begin{align*}
			p(\Tmat | \phivec, \Wmat) &= \prod_{n=1}^N \prod_{k=1}^K p(\class_k | \phivec_n)^{t_{nk}} \\\\
			&= \prod_{n=1}^N \prod_{k=1}^K \lp y_k(\phivec) \rp^{t_{nk}} \\\\
			&= \prod_{n=1}^N \prod_{k=1}^K \lp \frac{\exp(a_k)}{\sum_i^K \exp(a_i)} \rp^{t_{nk}} 
		\end{align*}
		
		The log-likelihood is written as:
		\begin{align*}
			\ln p(\Tmat | \phivec, \Wmat) &= \ln \lp \prod_{n=1}^N \prod_{k=1}^K p(\class_k | \phivec_n)^{t_{nk}} \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \ln y_k(\phivec) \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \ln \frac{\exp(a_k)}{\sum_i^K \exp(a_i)} \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \ln \exp(a_k) - \ln \sum_i^K \exp(a_i) \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \lp a_k - \ln \sum_i^K \exp(a_i) \rp \rp
		\end{align*}
		
		\item Derive the gradient of the log-likelihood with respect to $\wvec_{j}$. \\
		\emph{Solution:} \\
		
		\begin{align*}
			\frac{\partial \ln p(\Tmat | \phivec, \Wmat)}{\partial \wvec_j} &= \frac{\partial \lp \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \ln y_k(\phivec_n) \rp \rp}{\partial \wvec_j} \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \frac{1}{y_k(\phivec_n)} \frac{\partial y_k(\phivec_n)}{\partial \wvec_j} \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp \frac{t_{nk}}{y_k(\phivec_n)} y_k(\phivec_n) \lp \eye_{kj} - y_j(\phivec_n) \rp \phivec_n \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \lp \eye_{kj} - y_j(\phivec_n) \rp \phivec_n \rp \\\\
			&= \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} \eye_{kj} \phivec_n \rp - \sum_{n=1}^N \sum_{k=1}^K \lp t_{nk} y_j(\phivec_n) \phivec_n \rp \\\\
			&= \sum_{n=1}^N \phivec_n \sum_{k=1}^K \lp t_{nk} \eye_{kj} \rp - \sum_{n=1}^N y_j(\phivec_n) \phivec_n \sum_{k=1}^K t_{nk} \\\\
			&= \sum_{n=1}^N \phivec_n  t_{nj} - \sum_{n=1}^N y_j(\phivec_n) \phivec_n \\\\
			&= \sum_{n=1}^N \lp \lp  t_{nj} -  y_j(\phivec_n) \rp \phivec_n \rp
		\end{align*}
		
		\item What is the objective function we minimize that is equivalent to maximizing the log-likelihood? \\
		\emph{Solution:} \\
		The objective function is the \textit{cross-entropy error} $(E(\textbf{W}))$ and is equal to the negative log-likelihood. I.e.: 
		\begin{align*}
			E(\textbf{W}) &= -\ln p(\textbf{T}|\boldsymbol{\Phi},\textbf{W}) = - \sum_{n=1}^{N}\sum_{k=1}^{K} t_{nk} \ln y_k(\phi_n)
		\end{align*}
		\\
		Sometimes we may write $y_{nk}$ for $y_k(\phi_n)$; it is useful sometimes to give clutter-free solutions. Minimizing the cross-entropy requires the same gradients as maximizing the log-likelihood, except there is a change in sign:
		\begin{align*}
			\frac{\partial E(\textbf{W})}{\partial \textbf{w}_j} &= \sum_{n=1}^{N}\sum_{k=1}^{K} (y_{nj} - t_{nj})\phi_n \\
			&= \sum_{n=1}^{N} e_n
		\end{align*}
		
		\item Write a stochastic gradient algorithm for logistic regression using this objective function. Make sure to include indices for time and to define the learning rate. The gradients may differ in sign switching from maximizing to minimizing; don't overlook this.\\
		\emph{Solution:} \\
		The single step update for SGD is:		    
		\begin{align*}
			w_j^{t+1} &= w_j^{t} - \eta^{t} \nabla e_n
		\end{align*}
		
		\begin{enumerate}[label=(\alph*)]
			\item Initialize \textbf{W}
			\item Initialize $\eta$
			\item For $\textit{t} = 1$ to \textit{T} do:
			\begin{enumerate}[label = (\roman*)]
				\item Randomly choose \textit{n} from 			 				[1,\textit{N}]
				\item $\textbf{w}_j = \textbf{w}_j - \eta\nabla 						e_n(for all \textit{j})$
				\item Decrease $\eta$
			\end{enumerate}
			\item Return \textbf{W}
		\end{enumerate}
		
		\begin{comment}
		
		\item Explain why is this a stochastic optimization procedure?. \\
		\emph{Solution:} \\
		This is a stochastic procedure because we choose a random data vector $n$. This causes the full gradient (over all the data vectors) to be replaced by the gradient of the single random vector. Thus, every step introduces noise. \\
		Nonetheless, it is an optimizing procedure because the algorithm is guaranteed to converge to a local minimum, given enough steps ($T \to \infty $) and a process for annealing the learning rate to $0$.
		
		\item Logistic regression is not free from overfitting. How would you modify the cross-entropy error to regularize your weights? Write down the new objective. If we optimized for $\wvec$, would this be the maximum likelihood estimator or the maximum-a-posterior estimator? \\
		\emph{Solution:} \\
		In order to regularize the cross-entropy error we can use the regularization methods for the log-likelihood. This last one penalizes large values of $\Wmat$ by subtracting the regularization term:
		\begin{align*}
		\frac{\lambda}{2} \norm{\Wmat}^2
		\end{align*}
		
		Since the cross entropy error is the negative log-likelihood, the regularization needs to \texttt{ADD} the same term. The new objective is: 
		
		\begin{align*}
		E(\textbf{W}) &= - \sum_{n=1}^{N}\sum_{k=1}^{K} t_{nk} \ln y_k(\phi_n) + \frac{\lambda}{2} \norm{\Wmat}^2 
		\end{align*}
		
		Optimizing the cross entropy for $\Wmat$ is equivalent to optimizing the log-likelihood, which is also the log posterior. As such, optimizing the previous objective can be seen as the maximum-a-posterior estimator. 
		
		\end{comment}
	\end{enumerate}
	
\end{document}