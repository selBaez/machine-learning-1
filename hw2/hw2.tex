\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage[compact,explicit]{titlesec}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\loglikelihood}{loglikelihood}
\newcommand{\bishop}[1]{Bishop #1}
\newcommand{\newword}[1]{{\bf #1}}
\newcommand{\Data}{\mathcal{D}}
\newcommand{\Model}{\mathcal{M}}
\newcommand{\DataTrain}{\mathcal{D}_{{\text train}}}
\newcommand{\DataTest}{\mathcal{D}_{{\text test}}}
\newcommand{\N}{N}
\newcommand{\Ntrain}{\N_{train}}
\newcommand{\Ntest}{\N_{test}}
\newcommand{\DataSize}{\N}
\newcommand{\DataIndex}{n}
\newcommand{\cind}{t}
\newcommand{\pind}{\star}

\newcommand{\eye}{{\bf I}}

\newcommand{\Dim}{D}
\newcommand{\DimIndex}{d}

\newcommand{\DimOut}{K}
\newcommand{\DimOutIndex}{k}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\ED}{\mathbb{E}_{\Data}}
\DeclareMathOperator*{\V}{\mathbb{V}}
\DeclareMathOperator*{\LL}{L}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\trace{tr}
\DeclareMathOperator\median{median}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\expectation}{\E}
\newcommand{\expectationdata}{\ED}
\newcommand{\variance}{\V}
\newcommand{\loss}{\LL}
\newcommand{\ascalar}{a}
\newcommand{\xscalar}{x}
\newcommand{\xvec}{{\bf \xscalar}}
\newcommand{\xvectest}{\xvec_{\star}}
\newcommand{\Xmat}{{\bf \MakeUppercase\xscalar}}
\newcommand{\tscalar}{t}
\newcommand{\ttest}{\tscalar_{\star}}
\newcommand{\tvec}{{\bf \tscalar}}
\newcommand{\Tmat}{{\bf \MakeUppercase\tscalar}}
\newcommand{\yscalar}{y}
\newcommand{\yvec}{{\bf \yscalar}}
\newcommand{\Ymat}{{\bf \MakeUppercase\yscalar}}
\newcommand{\wscalar}{w}
\newcommand{\wvec}{{\bf \wscalar}}
\newcommand{\wvecML}{\wvec_{\text{MLE}}}
\newcommand{\wvecMAP}{\wvec_{\text{MAP}}}
\newcommand{\wvecs}{\wvec^{(s)}}
\newcommand{\Wmat}{{\bf \MakeUppercase\wscalar}}
\newcommand{\wbias}{\wscalar_0}
\newcommand{\xn}{\xscalar_{\DataIndex}}
\newcommand{\xvecn}{\xvec_{\DataIndex}}
\newcommand{\tvecn}{\tvec_{\DataIndex}}
\newcommand{\tn}{\tscalar_{\DataIndex}}
\newcommand{\ti}{\tscalar_{i}}
\newcommand{\yvecn}{\yvec_{\DataIndex}}
\newcommand{\yn}{\yscalar_{\DataIndex}}
\newcommand{\yfunc}{\yscalar}
\newcommand{\yfunctest}{\yfunc_{\star}}

\newcommand{\zerovec}{ {\bf 0}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\muvecN}{\muvec_{\N}}
\newcommand{\munotvec}{\boldsymbol{\mu}_0}
\newcommand{\mnot}{{\bf m_0}}
\newcommand{\mN}{{\bf m}_{\N}}
\newcommand{\mNplus}{{\bf m}_{\N+1}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\thetav}{\thetavec}
\newcommand{\phivec}{\boldsymbol{\phi}}
\newcommand{\phivectest}{\phivec_{\star}}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\phivv}{\phivec}
\newcommand{\phivecn}{\phivec_{\DataIndex}}
\newcommand{\phiveci}{\phivec_{i}}
\newcommand{\Sigmamat}{\boldsymbol{\Sigma}}
\newcommand{\SigmamatN}{\Sigmamat_N}
\newcommand{\Sigmamatnot}{\Sigmamat_0}
\newcommand{\SigmamatInv}{\Sigmamat^{-1}}
\newcommand{\Smat}{{\bf S}}
\newcommand{\SmatN}{\Smat_{\N}}
\newcommand{\Smatnot}{\Smat_0}
\newcommand{\SmatInv}{\Smat^{-1}}
\newcommand{\SmatNplus}{\Smat_{\N+1}}
\newcommand{\SmatNplusinv}{\Smat_{\N+1}^{-1}}

\newcommand{\betatest}{\beta_{\star}}
\newcommand{\Amat}{{\bf A}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}{{\bf E}}
\newcommand{\Fmat}{{\bf F}}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}{{\bf J}}
\newcommand{\Kmat}{{\bf K}}
\newcommand{\Lmat}{{\bf L}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}{{\bf N}}
\newcommand{\Omat}{{\bf O}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}{{\bf Q}}
\newcommand{\Rmat}{{\bf R}}
%\newcommand{\Smat}{{\bf S}}
%\newcommand{\Tmat}{{\bf T}}
\newcommand{\Umat}{{\bf U}}
\newcommand{\Vmat}{{\bf V}}
%\newcommand{\Wmat}{{\bf W}}
%\newcommand{\Xmat}{{\bf X}}
%\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}



\newcommand{\rvec}{{\bf r}}
\newcommand{\dvec}{{\bf d}}
\newcommand{\lvec}{{\bf l}}
\newcommand{\mvec}{{\bf m}}
\newcommand{\uvec}{{\bf u}}
\newcommand{\vvec}{{\bf v}}

\newcommand{\xvecmean}{\bar{\xvec}}
\newcommand{\xvecnest}{\tilde{\xvec}_n}
\newcommand{\xvecestn}{\xvecnest}
\newcommand{\class}{\mathcal{C}}
\newcommand{\sigmoid}{\sigma}

\newcommand{\ans}[1]{ 
  \begin{center}
    % \fbox{
      \begin{minipage}{.8\textwidth}
        {\sf#1}
      \end{minipage}
    % }
  \end{center} 
}

\title{Machine Learning 1 - Homework 2}
\author{Selene Baez Santamaria}

%\author{Monday, September 12, 2016 \\ Deadline: Wednesday, September 21, 2016, 23:59}
\date{}
\begin{document}
\maketitle
\titleformat{\section}[runin]{\large\bfseries}{}{0pt}{\thesection\quad\underline{#1}\vspace{0.1in}\newline}
\titleformat{\subsection}[runin]{\normalsize\bfseries}{}{0pt}{#1 \thesubsection\newline}

\vspace{0.25in}
\section{MAP solution for Linear Regression}
In class we solved for the maximum likelihood estimator for linear regression with polynomial basis functions.  In this exercise you will solve for the {\em maximum a posterior} (MAP) solution: $\wvecMAP = \lp \Phimat^T \Phimat + \lambda \eye\rp^{-1} \Phimat^T \tvec$.  For this problem we assume $N$ training vectors $\{ \xvecn \}_{n=1}^N$, each of which is mapped using basis functions to $\phivecn$.  In the training set, the data come in input-output pairs, i.e. $\{\xvecn, \tn\}$.  We assume that one of the basis functions is the constant $1$, and there are $M-1$ other basis function in $\phivecn$.  We also have the following information:
\begin{itemize}
  \item The regression prediction: $\yfunc(\xvecn, \wvec) = \wvec^T\phivecn$.
  \item The likelihood function: $p( \tn | \phivecn, \wvec, \beta ) = \mathcal{N}\lp \tn | \wvec^T\phivecn, 1/\beta \rp$
  \item The prior over $\wvec$: $p(\wvec) = \mathcal{N}\lp \wvec | \zerovec, \eye/\alpha\rp$.  $\eye$ is the identity matrix, $\zerovec$ is a vector of $0$'s.
  \item The data are iid (independently and identically distributed).
\end{itemize}
Answer the following:
\begin{enumerate}
  \item Write down the likelihood $p(\Data | \thetav)$ using a) a product over $N$ and b) in vector/matrix form.    Tip: You can answer both a) and b) in one set of equations by starting with a), then simplifying to get b).  For b) make sure to define any matrices and vectors. \\
	  \emph{Solution:} \\
		  \begin{align*}
		  p(\Data | \thetav) &= \prod_{n=1}^{N} \mathcal{N}(\tn| \wvec^T \phivecn, 1 / \beta) \\
		  &=  \prod_{n=1}^{N} \frac{\beta^{1/2}}{(2\pi)^{1/2}} \exp (-\frac{\beta}{2}(\tn- \wvec^T \phivecn)^2)\\
		  &=  \frac{\beta^{N/2}}{(2\pi)^{N/2}} \prod_{n=1}^{N} \exp (-\frac{\beta}{2}(\tn- \wvec^T \phivecn)^2)\\    
		  &=  \frac{\beta^{N/2}}{(2\pi)^{N/2}} \exp (-\frac{\beta}{2}\sum(\tn - \wvec^T \phivecn)^2)\\    
		  &=  \frac{\beta^{N/2}}{(2\pi)^{N/2}} \exp (-\frac{\beta}{2}(\tvec - \Phimat \wvec)^T(\tvec - \Phimat \wvec))\\
		  &=\mathcal{N}(\tvec | \Phimat \wvec, \beta^{-1} \eye )
		  \end{align*}
	  
  \item Write down the prior $p(\wvec)$ (by expanding the expression for multivariate Gaussian distribution).  Compute its log.\\
	  \emph{Solution:} \\
		  \begin{align*}
		  p(\wvec) &= \mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha}\eye) \\
		  &= \frac{\alpha^{D/2}}{(2\pi)^{D/2}} \exp (- \frac{\alpha}{2} \wvec^T \wvec)
		  \end{align*}
		  
		  \begin{align*}
		  \ln p(\wvec) &= \frac{D}{2} \ln \alpha - \frac{D}{2} \ln (2 \pi) - \frac{\alpha}{2}\wvec^T \wvec \\
		  &= \frac{\alpha}{2} \wvec^T \wvec + \mathcal{C}
		  \end{align*}  
  
  \item Write down an expression for the posterior over $\wvec$.  Remember this will involve applying Bayes rule to the prior, likelihood, and evidence.  The evidence will require an integral.  You do not need the analytic form for the evidence, but you need the correct variables and conditioning variables, e.g. something like $p(a|b,c)$ where you define $a$, $b$, and $c$.\\
	  \emph{Solution:} \\
		  \begin{align*}
		  p(\wvec | \Data) &= \frac{\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \prod_{n=1}^{N} \mathcal{N}(\tn| \wvec^T \phivecn, 1 / \beta)}{\int \mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \prod_{n=1}^{N} \mathcal{N}(\tn| \wvec^T \phivecn, 1 / \beta) d\wvec} \\
		  &= \frac{\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye)}{\int {\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye) d\wvec}} \\
		  &= \frac{\mathcal{N}(\wvec | \zerovec, \frac{1}{\alpha} \eye) \mathcal{N} (\tvec | \Phimat \wvec, \frac{1}{\beta} \eye)}{p(t | \Phimat, \alpha, \beta)}
		  \end{align*}
  
  \item Compute the log-posterior, both for the a) and b) likelihood forms from above.  Collect everything that does not depend on $\wvec$ into a constant $C$.  What parts of the previous expression do not depend on $\wvec$?  Why is finding the MAP much simpler than finding the full posterior distribution? \\
	  \emph{Solution:} \\
		  \begin{align*}
		  \ln p(\wvec | \Data) &= - \frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} \sum_{n=1}^{N}(\tn - \wvec^T \phivecn)^{2} + \mathcal{C} \\
		  &= \frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} (\tvec - \Phimat \wvec)^{T} (\tvec - \Phimat \wvec) + \mathcal{C}
		  \end{align*}
  
  \item Solve for $\wvecMAP$ by a) taking the derivative of the log-posterior with respect to $\wvec$, b) setting it to 0, and c) solving for $\wvec$.  Do this for both forms of likelihood.\\
	  \emph{Solution:} \\
		  \begin{align*}
		  \ln p(\wvec | \Data) &= \frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} \sum_{n=1}^{N}(\tn- \wvec^T \phivecn)^{2} + \mathcal{C}
		  \end{align*}
		  
		  \begin{align*}
		  \frac{\partial \ln p(\wvec | \Data)}{\partial \wvec} &= - \alpha \wvec - \beta \sum_{n=1}^{N} (\tn- \wvec^T \phivecn)(-\phivecn) &= 0
		  \end{align*}
		  
		  \begin{align*}
		  \alpha \wvec = \beta \sum_{n=1}^{N} (\tn- \wvec^T \phivecn) \phivecn
		  \end{align*}
		  
		  \begin{align*}
		  \alpha \wvec = \beta \sum_{n=1}^{N} \tn \phivecn - \underbrace{\wvec^T \phivecn}_{\text{scalar}} \phivecn
		  \end{align*}
		  
		  \begin{align*}
		  \alpha \wvec = \beta \sum_{n=1}^{N} \tn \phivecn - \phivecn \underbrace{\wvec^T \phivecn}_{\text{same as } \phivecn^{T} \wvec}
		  \end{align*}
		  
		  \begin{align*}
		  (\alpha \eye + \beta \sum_{n=1}^{N} \phivecn\phivecn^T) \wvec = \beta \sum_{n=1}^{N} \tn \phivecn 
		  \end{align*}
		  
		  \begin{align*}
		  \wvecMAP = (\alpha \eye + \beta \sum_{n=1}^{N} \phivecn\phivecn^T)^{-1} \beta \sum_{n=1}^{N} t_n \phivecn
		  \end{align*}
		  
		  \begin{align*}
		  \ln{p(\wvec | \Data)} &= -\frac{\alpha}{2} \wvec^T \wvec - \frac{\beta}{2} (\tvec - \Phimat \wvec)^T (\tvec - \Phimat \wvec) + \mathcal{C} \\
		  &= - \frac{\alpha}{2} \wvec^T\wvec - \frac{\beta}{2}\wvec^T \Phimat^T \Phimat \wvec + \beta \wvec^T \Phimat^T \tvec + \mathcal{D}
		  \end{align*}
		  
		  \begin{align*}
		  \frac{\partial \ln p(\wvec | D)}{\partial \wvec} &= - \alpha \wvec - \beta \mathbf{\Phi}^T\mathbf{\Phi} \wvec + \beta \mathbf{\Phi}^T \tvec = 0 
		  \end{align*}
		  
		  \begin{align*}
		  (\alpha\mathcal{I} + \beta \mathbf{\Phi}^T\mathbf{\Phi})\wvec = \beta\mathbf{\Phi}^T\tvec
		  \end{align*}
		  
		  \begin{align*}
		  \wvecMAP &= (\alpha \mathcal{I} + \beta \mathbf{\Phi}^T\mathbf{\Phi})^{-1} \beta\mathbf{\Phi}^T\tvec\\
		  &=(\beta (\frac{\alpha}{\beta}\mathcal{I} + \mathbf{\Phi}^T\mathbf{\Phi}))^{-1} \beta\mathbf{\Phi}^T \tvec\\
		  &= \frac{1}{\beta}(\lambda \mathbf{I} + \mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\tvec
		  \end{align*}
  
  \item {\bf BONUS}~~~Our prior for $\wvec$ assumes the same marginal distribution for each entry in $\wvec$, including that of the first basis function $\phivec_0 = 1$.  What is the role this basis function?  Why should we avoid placing the same penalty/prior for this basis?  Rewrite $p(\wvec)$ so that the first basis function has its own prior/penalty. \\
	  \emph{Solution:} \\
  
  
\end{enumerate}

\vspace{0.25in}
\section{Probability distributions, likelihoods, and estimators}
For these questions you will be working with different probability density functions listed in the table below.  The purpose of these questions is to practice working with a variety of PDFs and to make computing likelihoods, MLEs, etc. more natural. Note below the {\em indicator} notation $[x=0]$ (and $[x=1]$).  The square brackets evaluate to 1 if the argument is true, and 0 otherwise.  E.g. if $x$ is $1$, the $[x=0] = 0$ and $[x=1] = 1$ (here $[x=0]$ is lazy notation; in Python you would write $x==0$, for example).  We will use the notation a lot, both below and when we learn about classification.
%
\begin{table}[h!]
  \centering
\begin{tabular}{|l|c|c|c|}
  \hline
  Distribution & $p(x | \theta )$ &   Range of x & Range of $\theta$ \\ \hline\hline
  Bernouilli & $\theta^{[x=1]}(1-\theta)^{[x=0]}$ & $x \in \{0,1\}$ & $0 \leq \theta \leq 1$ \\\hline
  Beta & $\frac{\Gamma\lp\theta_1+\theta_0\rp}{\Gamma\lp\theta_1\rp\Gamma\lp\theta_0\rp} x^{\theta_1-1}(1-x)^{\theta_0-1}$ & $0 \leq x \leq 1$ & $\theta_1 > 0, \theta_0 > 0$ \\ \hline
  %
  Poisson & $\frac{\theta^x}{x!}e^{-\theta}$& $x \in \{0,1,2,\ldots\}$ & $\theta > 0$ \\ \hline
  %
  Gamma & $\frac{\theta_1^{\theta_0}}{\Gamma\lp\theta_0\rp} x^{\theta_0-1}e^{-\theta_1 x}$& $x \geq 0$ & $\theta_1 \geq 0$, $\theta_0 \geq 0$ \\ \hline
  %
  Gaussian & $\frac{1}{\sqrt{2\pi\theta_1}} e^{-\frac{1}{2}\lp \frac{x-\theta_0}{\theta_1}\rp^2}$ & $-\infty < x < \infty$ &  $-\infty < \theta_0 < \infty$,  $\theta_1>1$  \\ \hline
\end{tabular}
\end{table}

\subsection{Question}
For each of the probability distributions above, write down their normalizing constants.  Remember that $\int p(x|\theta) d x = 1$ for continuous $x$ and $\sum_x p(x|
\theta) = 1$ for discrete $x$.

\subsection{Question}
You live in Amsterdam and find that it rains quite a lot.  You want to estimate the probability that it will rain any given day of the year.  Every month for a year you count the number of days with rain, and you get the following (from January to December): 22,19,16,16,14,14,17,18,19,20,21,21 (for a grand total of 217 days with rain).\footnote{Source: \url{http://www.amsterdam.climatemps.com/}.}  Let $r_t$ be an observation for day $t$ in the year; $r_t=1$ means there was some rain on day $t$, $r_t=0$ means there was no rain.  We want to estimate the parameter $\rho$, the probability of rain on any day of the year.  We assume a Bernouilli distribution for the observations $\{r_t\}_{t=1}^{365}$, that is $p( r_t | \rho ) = \text{Bernouilli}(r_t | \rho)$.  To answer these questions, the number of days of rain per month is not important, only the total for the year is relevant.  With this information, answer the following questions:
\begin{enumerate}
  \item What is the likelihood for a single observation?  For the entire set of observations? 
  \item Write the log-likelihood for the entire set of observations.  
  \item Solve for the MLE of $\rho$.  Do it in general (with symbols for counts $n_0$, $n_1$ for days without and with rain) and for this specific case (plug-in the numbers).
  \item Assume a Beta prior for $\rho$ with parameters $a$ and $b$.  What is the MAP for $\rho$?
  \item Write the form of the posterior distribution for $\rho$?  You do not need to solve it analytically.
  \item (Optional) Solve for the posterior distribution analytically.  Hint: it is a Beta distribution.
\end{enumerate}

\subsection{Question}
You work in the staffing department of a maternity hospital and part of your job is to determine the staffing requirements during the night shift at your hospital.  This might mean the number of doctors and nurses at the hospital and the number of doctors on call (if there are more than the average number of deliveries).  Your goal is to determine the distribution over the number of deliveries during the night shift $d_t \in \{0,1,2,\ldots\}$ ($d$ for delivery count, $t$ for time, the index of the night).  With this you can compute the mean, the probability of more than $5$ deliveries, etc.  You collect data for two weeks, i.e. $d_1, \ldots, d_{14} = 4, 7, 3, 0, 2, 2, 1, 5, 4, 4, 3, 3, 2, 3$.  You assume the observations are explained by a Poisson distribution with parameter $\lambda$ over the discrete delivery counts. With this information, answer the following questions:
%
\begin{enumerate}
  \item What is the likelihood for a single observation?  For the entire set of observations? 
  \item Write the log-likelihood for the entire set of observations.  
  \item Solve for the MLE of $\lambda$.  Do it in general and for this specific case (plug-in the numbers).
  \item Assume a Gamma prior for $\lambda$ with parameters $a$ and $b$.  What is the MAP estimate of $\lambda$?
  \item Write the form of the posterior distribution for $\lambda$? (You do not need to solve it analytically)
  \item (Optional) Solve for the posterior distribution analytically.  Hint: it is a Gamma distribution.
\end{enumerate}

\subsection{Question}
You have developed a blood test aimed at detecting a disease $d \in \{0,1\}$ (disease is absent ($d=0$) or present ($d=1$)).  The test measures the level of a specific indicator of the disease, that is it returns a real valued number relative to some baseline (so the levels can be both negative and positive -- anywhere along the real line).  Two models of the population are built: one for the patients with the disease, and another for the general population.  Measurements tend to have a Gaussian shape, and we therefore model the entire population as a mixture of two Gaussians.  That is, $p( l ) = p(d=0) p( l | d=0 ) + p(d=1) p( l | d=1 )$, where $p(d)$ is the prior distribution of patients with and without the disease in the general population and $p(l|d)$  are conditional Gaussian distributions, one for the patients with disease, and one for those without.  Note: with this question and the previous two, we are simply applying rules of probability (with some algebra) to get the form of the posterior distribution; however, in this problem we are also classifying (since our target is the discrete label $d$).

Assume we know $p(d=0) = \pi_0 = 0.999$ and $p(d=1) = \pi_1 = 0.001$ from previous experience.  We do not know the parameters $\mu_0, \sigma_0^2$ (the mean and variance of the disease-free population) nor $\mu_1, \sigma_1^2$ (for the disease population).  We measure levels $\{ l_n\}_{n=1}^N$ for N people, and we know that $n\in\{D_0\}$ are the indices for the disease free patients and $n\in\{D_1\}$ are the indices for the patients with the disease (i.e. $D_0$ and $D_1$ are non-intersecting sets of indices from $1$ to $N$).  With this information, answer the following questions:
\begin{enumerate}
  \item Write down the likelihood of the observations as a product over $N$ level recordings.  Hint: use indicator notation (like in the Bernouilli distribution) to distinguish between $d_n=0$ and $d_n=1$ in the likelihood.
  \item Write down the likelihood as a product over the likelihoods for $\{D_0\}$ and $\{D_1\}$.
  \item Compute the log-likelihood.
  \item Find the MLE for $\mu_0$ and $\sigma_0^2$.  Assume we can do the same for $\mu_1$ and $\sigma_1^2$
  \item We now have our models.  To make a prediction, solve for $p( d=1 | l_{\star})$, where $l_{\star}$ is a level recorded for a new patient.  Hint: use Bayes theorem.
  \item Reduce your solution to have the form of a sigmoid, i.e. 
      \begin{equation*}
         p(d=1|l_{\star}) = \frac{1}{1+e^{-a(l_{\star})}}.
      \end{equation*}
  
\end{enumerate}

\end{document}